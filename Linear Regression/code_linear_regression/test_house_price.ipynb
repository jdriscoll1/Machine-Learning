{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "706c40c4-f940-496e-9e3b-2bfa4d128c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from numpy.random import rand as rand\n",
    "from numpy.random import seed as seed\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#import linear_regression as LR\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append('..')\n",
    "#from misc.utils import MyUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32f3e390-80da-4ef3-bd0b-cb4161277d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "(404, 1)\n",
      "(102, 13)\n",
      "(102, 1)\n"
     ]
    }
   ],
   "source": [
    "df_X_train = pd.read_csv('houseprice/x_train.csv', header=None)\n",
    "df_y_train = pd.read_csv('houseprice/y_train.csv', header=None)\n",
    "df_X_test = pd.read_csv('houseprice/x_test.csv', header=None)\n",
    "df_y_test = pd.read_csv('houseprice/y_test.csv', header=None)\n",
    "\n",
    "X_train = df_X_train.to_numpy()\n",
    "X_test = df_X_test.to_numpy()\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "X_all = MyUtils.normalize_0_1(np.concatenate((X_train, X_test), axis=0))\n",
    "X_train = X_all[:n_train]\n",
    "X_test = X_all[n_train:]\n",
    "\n",
    "y_train = df_y_train.to_numpy()\n",
    "y_test = df_y_test.to_numpy()\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "#print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18f3e85e-29ad-4d03-bf3a-5e3379162632",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_r = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a66944ba-0382-4243-b4b6-5800d440bd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560, 1)\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train, CF = True, lam = 1, eta = 0.005, epochs = 10000, degree = 3)\n",
    "print(lr.w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30c187b9-a52f-4dbe-80fd-6d3a37933e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.47320962e-03]\n",
      " [1.15628146e-04]\n",
      " [6.79842548e-04]\n",
      " [1.52627888e-03]\n",
      " [3.51544912e-04]\n",
      " [1.39162716e-03]\n",
      " [2.48977008e-03]\n",
      " [2.85258440e-03]\n",
      " [1.14833532e-03]\n",
      " [1.37996742e-03]\n",
      " [1.60627838e-03]\n",
      " [2.58346324e-03]\n",
      " [4.14817702e-03]\n",
      " [1.09003364e-03]\n",
      " [2.61933407e-05]\n",
      " [5.45948269e-07]\n",
      " [7.22962876e-05]\n",
      " [8.61413793e-06]\n",
      " [6.63228082e-05]\n",
      " [5.33785219e-05]\n",
      " [1.05575604e-04]\n",
      " [8.33987020e-06]\n",
      " [1.04003469e-04]\n",
      " [9.72533408e-05]\n",
      " [8.83654378e-05]\n",
      " [8.12887556e-05]\n",
      " [5.16124942e-05]\n",
      " [4.30453335e-04]\n",
      " [7.27089322e-05]\n",
      " [3.89214971e-05]\n",
      " [6.50538728e-05]\n",
      " [4.39404345e-04]\n",
      " [2.24422134e-04]\n",
      " [3.17100267e-04]\n",
      " [8.18195270e-05]\n",
      " [1.40468724e-04]\n",
      " [2.67542428e-04]\n",
      " [6.67285214e-04]\n",
      " [7.79409066e-05]\n",
      " [7.92296020e-04]\n",
      " [1.49772803e-04]\n",
      " [6.62689612e-04]\n",
      " [7.78764832e-04]\n",
      " [1.17593873e-03]\n",
      " [2.55071483e-04]\n",
      " [6.92573823e-04]\n",
      " [7.73202913e-04]\n",
      " [9.86318668e-04]\n",
      " [1.34138048e-03]\n",
      " [4.82074830e-04]\n",
      " [3.51544912e-04]\n",
      " [1.33151453e-04]\n",
      " [2.08776724e-04]\n",
      " [2.62397126e-04]\n",
      " [6.43163831e-05]\n",
      " [1.31675919e-04]\n",
      " [1.33510200e-04]\n",
      " [1.83081212e-04]\n",
      " [3.31755142e-04]\n",
      " [7.95605398e-05]\n",
      " [6.60867860e-04]\n",
      " [7.26219947e-04]\n",
      " [1.10476075e-03]\n",
      " [2.14382189e-04]\n",
      " [6.36540501e-04]\n",
      " [6.92548925e-04]\n",
      " [8.50522967e-04]\n",
      " [1.21725424e-03]\n",
      " [4.37834554e-04]\n",
      " [1.48943332e-03]\n",
      " [1.55228659e-03]\n",
      " [6.59758415e-04]\n",
      " [7.04525318e-04]\n",
      " [8.24429215e-04]\n",
      " [1.36072585e-03]\n",
      " [2.33236587e-03]\n",
      " [5.29337319e-04]\n",
      " [2.20029730e-03]\n",
      " [5.47502121e-04]\n",
      " [1.07775453e-03]\n",
      " [1.20725394e-03]\n",
      " [1.72481286e-03]\n",
      " [2.58158517e-03]\n",
      " [8.14793273e-04]\n",
      " [4.45537649e-04]\n",
      " [2.20436312e-04]\n",
      " [2.95554220e-04]\n",
      " [6.21691884e-04]\n",
      " [1.10512859e-03]\n",
      " [2.18715081e-04]\n",
      " [9.40270019e-04]\n",
      " [9.00352492e-04]\n",
      " [9.61624779e-04]\n",
      " [1.15292098e-03]\n",
      " [4.49035592e-04]\n",
      " [9.62141171e-04]\n",
      " [1.06549511e-03]\n",
      " [1.38024589e-03]\n",
      " [5.01302515e-04]\n",
      " [1.75939695e-03]\n",
      " [2.36216021e-03]\n",
      " [7.05302211e-04]\n",
      " [3.99914202e-03]\n",
      " [9.54859177e-04]\n",
      " [4.12312504e-04]\n",
      " [1.34835208e-05]\n",
      " [1.92955922e-09]\n",
      " [1.69282735e-05]\n",
      " [5.88172858e-07]\n",
      " [1.52289473e-05]\n",
      " [1.20733385e-05]\n",
      " [2.47757009e-05]\n",
      " [1.17871845e-06]\n",
      " [2.60262087e-05]\n",
      " [2.38339907e-05]\n",
      " [2.10833906e-05]\n",
      " [1.59415716e-05]\n",
      " [1.30686646e-05]\n",
      " [2.01898206e-07]\n",
      " [7.43866182e-08]\n",
      " [2.26303401e-08]\n",
      " [1.48986326e-07]\n",
      " [3.59666488e-07]\n",
      " [2.97575819e-07]\n",
      " [1.73148132e-07]\n",
      " [8.97343637e-08]\n",
      " [1.04063572e-07]\n",
      " [1.65912138e-07]\n",
      " [5.34811259e-07]\n",
      " [8.51532288e-08]\n",
      " [4.64468223e-05]\n",
      " [5.46914645e-06]\n",
      " [4.23064584e-05]\n",
      " [3.30094324e-05]\n",
      " [6.67370442e-05]\n",
      " [4.70492192e-06]\n",
      " [6.68022917e-05]\n",
      " [6.24700713e-05]\n",
      " [5.57656075e-05]\n",
      " [5.01481237e-05]\n",
      " [3.28700800e-05]\n",
      " [8.61413793e-06]\n",
      " [5.13633946e-06]\n",
      " [4.74140980e-06]\n",
      " [7.98353599e-06]\n",
      " [3.37260847e-07]\n",
      " [7.26435163e-06]\n",
      " [6.94159414e-06]\n",
      " [6.14797739e-06]\n",
      " [7.83889398e-06]\n",
      " [1.65585626e-06]\n",
      " [4.04149759e-05]\n",
      " [3.03937589e-05]\n",
      " [6.13857768e-05]\n",
      " [4.28782313e-06]\n",
      " [6.06782734e-05]\n",
      " [5.67940914e-05]\n",
      " [5.05464464e-05]\n",
      " [4.65786062e-05]\n",
      " [3.07278117e-05]\n",
      " [2.74794151e-05]\n",
      " [4.83466806e-05]\n",
      " [4.13742706e-06]\n",
      " [4.70807168e-05]\n",
      " [4.41272321e-05]\n",
      " [4.01763634e-05]\n",
      " [3.84817261e-05]\n",
      " [2.19674634e-05]\n",
      " [9.83923489e-05]\n",
      " [6.82594022e-06]\n",
      " [9.61566065e-05]\n",
      " [8.98700876e-05]\n",
      " [8.09274946e-05]\n",
      " [7.36562720e-05]\n",
      " [4.79987905e-05]\n",
      " [1.20074859e-06]\n",
      " [6.49659042e-06]\n",
      " [6.16536559e-06]\n",
      " [6.20288929e-06]\n",
      " [6.11001292e-06]\n",
      " [3.29001023e-06]\n",
      " [1.02054598e-04]\n",
      " [9.36341572e-05]\n",
      " [8.31620949e-05]\n",
      " [7.08481933e-05]\n",
      " [4.81588596e-05]\n",
      " [8.64784340e-05]\n",
      " [7.69206596e-05]\n",
      " [6.66368216e-05]\n",
      " [4.47813830e-05]\n",
      " [7.03349895e-05]\n",
      " [6.12521483e-05]\n",
      " [4.03339055e-05]\n",
      " [7.36563243e-05]\n",
      " [3.46046385e-05]\n",
      " [2.85632567e-05]\n",
      " [3.21982638e-04]\n",
      " [3.77319457e-05]\n",
      " [2.46867215e-05]\n",
      " [2.91172329e-05]\n",
      " [2.84805389e-04]\n",
      " [1.22969268e-04]\n",
      " [2.12176413e-04]\n",
      " [4.53442739e-05]\n",
      " [8.94588256e-05]\n",
      " [1.62405708e-04]\n",
      " [4.22640363e-04]\n",
      " [4.26290623e-05]\n",
      " [1.20890392e-05]\n",
      " [3.84173063e-06]\n",
      " [8.53128855e-06]\n",
      " [4.47461420e-05]\n",
      " [2.65953180e-05]\n",
      " [3.06501868e-05]\n",
      " [1.01744593e-05]\n",
      " [1.41641735e-05]\n",
      " [3.29243812e-05]\n",
      " [7.15240449e-05]\n",
      " [9.88860718e-06]\n",
      " [3.89214971e-05]\n",
      " [3.01253323e-06]\n",
      " [3.05214827e-05]\n",
      " [1.33253754e-05]\n",
      " [1.50491737e-05]\n",
      " [2.12953551e-06]\n",
      " [1.93938197e-06]\n",
      " [1.05072924e-05]\n",
      " [3.84071019e-05]\n",
      " [2.96213551e-06]\n",
      " [1.39548305e-05]\n",
      " [4.29169384e-05]\n",
      " [3.15547817e-05]\n",
      " [2.29308568e-05]\n",
      " [9.63364886e-06]\n",
      " [1.22563402e-05]\n",
      " [2.24796898e-05]\n",
      " [6.40329997e-05]\n",
      " [9.07012339e-06]\n",
      " [2.94021407e-04]\n",
      " [1.45618992e-04]\n",
      " [1.99823247e-04]\n",
      " [5.20037830e-05]\n",
      " [8.71604876e-05]\n",
      " [1.61922569e-04]\n",
      " [4.31563486e-04]\n",
      " [4.62213890e-05]\n",
      " [9.89586393e-05]\n",
      " [9.49911516e-05]\n",
      " [3.02943297e-05]\n",
      " [4.33219872e-05]\n",
      " [8.43950679e-05]\n",
      " [2.20548479e-04]\n",
      " [3.07786674e-05]\n",
      " [1.65609219e-04]\n",
      " [3.64689517e-05]\n",
      " [7.14676329e-05]\n",
      " [1.31847095e-04]\n",
      " [3.10305482e-04]\n",
      " [3.69855281e-05]\n",
      " [1.36951928e-05]\n",
      " [1.71553448e-05]\n",
      " [3.49084441e-05]\n",
      " [8.03201920e-05]\n",
      " [9.99239558e-06]\n",
      " [3.81064844e-05]\n",
      " [5.66566063e-05]\n",
      " [1.37553680e-04]\n",
      " [1.72938760e-05]\n",
      " [1.43379857e-04]\n",
      " [2.62882128e-04]\n",
      " [3.32387387e-05]\n",
      " [6.55417624e-04]\n",
      " [7.61823316e-05]\n",
      " [1.36943647e-05]\n",
      " [4.93170398e-04]\n",
      " [8.23677999e-05]\n",
      " [3.98587235e-04]\n",
      " [3.89607702e-04]\n",
      " [6.68607114e-04]\n",
      " [9.53685782e-05]\n",
      " [4.14576174e-04]\n",
      " [4.64666008e-04]\n",
      " [5.30336706e-04]\n",
      " [6.75268976e-04]\n",
      " [2.78924556e-04]\n",
      " [1.49772803e-04]\n",
      " [7.52000884e-05]\n",
      " [8.47760902e-05]\n",
      " [1.27261920e-04]\n",
      " [1.70178530e-05]\n",
      " [7.41162615e-05]\n",
      " [7.80176786e-05]\n",
      " [8.43123940e-05]\n",
      " [1.38413753e-04]\n",
      " [3.53072428e-05]\n",
      " [3.70669233e-04]\n",
      " [3.23748847e-04]\n",
      " [5.73266552e-04]\n",
      " [7.23640253e-05]\n",
      " [3.72315514e-04]\n",
      " [4.05514783e-04]\n",
      " [4.32645590e-04]\n",
      " [5.53088994e-04]\n",
      " [2.41057675e-04]\n",
      " [4.30049588e-04]\n",
      " [5.92489881e-04]\n",
      " [1.36289443e-04]\n",
      " [3.33717100e-04]\n",
      " [3.75623006e-04]\n",
      " [4.85978162e-04]\n",
      " [6.92077167e-04]\n",
      " [2.21190396e-04]\n",
      " [1.00797737e-03]\n",
      " [1.51052867e-04]\n",
      " [5.91010024e-04]\n",
      " [6.57406845e-04]\n",
      " [7.74321812e-04]\n",
      " [1.01217680e-03]\n",
      " [4.03075621e-04]\n",
      " [7.47948193e-05]\n",
      " [7.61198973e-05]\n",
      " [8.91890625e-05]\n",
      " [1.55731574e-04]\n",
      " [2.37620185e-04]\n",
      " [6.45431476e-05]\n",
      " [5.69662238e-04]\n",
      " [5.41325157e-04]\n",
      " [5.18519034e-04]\n",
      " [5.49844455e-04]\n",
      " [2.56148340e-04]\n",
      " [5.59144382e-04]\n",
      " [5.55111901e-04]\n",
      " [6.32550436e-04]\n",
      " [2.79205908e-04]\n",
      " [7.21320616e-04]\n",
      " [8.57029638e-04]\n",
      " [3.33776000e-04]\n",
      " [1.26885161e-03]\n",
      " [3.99785786e-04]\n",
      " [2.12536274e-04]\n",
      " [3.51544912e-04]\n",
      " [1.33151453e-04]\n",
      " [2.08776724e-04]\n",
      " [2.62397126e-04]\n",
      " [6.43163831e-05]\n",
      " [1.31675919e-04]\n",
      " [1.33510200e-04]\n",
      " [1.83081212e-04]\n",
      " [3.31755142e-04]\n",
      " [7.95605398e-05]\n",
      " [7.59202537e-05]\n",
      " [7.24666779e-05]\n",
      " [1.14422052e-04]\n",
      " [1.36328424e-05]\n",
      " [6.70133655e-05]\n",
      " [7.16907428e-05]\n",
      " [7.20474831e-05]\n",
      " [1.19704072e-04]\n",
      " [3.41324231e-05]\n",
      " [1.34501986e-04]\n",
      " [1.50018415e-04]\n",
      " [4.11390757e-05]\n",
      " [7.38503656e-05]\n",
      " [7.51560695e-05]\n",
      " [1.02572197e-04]\n",
      " [1.98354308e-04]\n",
      " [3.83874632e-05]\n",
      " [2.15352083e-04]\n",
      " [3.84454093e-05]\n",
      " [1.16000977e-04]\n",
      " [1.18719636e-04]\n",
      " [1.45350559e-04]\n",
      " [2.46296288e-04]\n",
      " [6.39684037e-05]\n",
      " [1.83579960e-05]\n",
      " [1.06498760e-05]\n",
      " [1.03030554e-05]\n",
      " [2.81586924e-05]\n",
      " [6.23507600e-05]\n",
      " [1.38464165e-05]\n",
      " [9.91710250e-05]\n",
      " [9.25071995e-05]\n",
      " [9.14368721e-05]\n",
      " [1.23124999e-04]\n",
      " [2.49068572e-05]\n",
      " [9.02855827e-05]\n",
      " [8.70062030e-05]\n",
      " [1.23528038e-04]\n",
      " [2.69860422e-05]\n",
      " [1.15187218e-04]\n",
      " [1.74008252e-04]\n",
      " [4.34638261e-05]\n",
      " [3.17900734e-04]\n",
      " [7.44071768e-05]\n",
      " [2.98872533e-05]\n",
      " [3.96379187e-04]\n",
      " [3.28035893e-04]\n",
      " [5.75074737e-04]\n",
      " [6.94035699e-05]\n",
      " [3.61076948e-04]\n",
      " [3.89341481e-04]\n",
      " [3.96653421e-04]\n",
      " [5.49562974e-04]\n",
      " [2.39946388e-04]\n",
      " [4.14973926e-04]\n",
      " [5.69661310e-04]\n",
      " [1.17353875e-04]\n",
      " [3.09829904e-04]\n",
      " [3.36868321e-04]\n",
      " [4.22612367e-04]\n",
      " [6.43792674e-04]\n",
      " [2.03144630e-04]\n",
      " [9.51810858e-04]\n",
      " [1.36932157e-04]\n",
      " [5.51699502e-04]\n",
      " [5.98260832e-04]\n",
      " [6.81176017e-04]\n",
      " [9.49313421e-04]\n",
      " [3.73650773e-04]\n",
      " [5.85381545e-05]\n",
      " [6.60775016e-05]\n",
      " [7.50669654e-05]\n",
      " [1.29289203e-04]\n",
      " [1.98375072e-04]\n",
      " [5.40459000e-05]\n",
      " [5.20061990e-04]\n",
      " [4.91421535e-04]\n",
      " [4.67634262e-04]\n",
      " [5.05717935e-04]\n",
      " [2.38292777e-04]\n",
      " [4.97958639e-04]\n",
      " [4.86548620e-04]\n",
      " [5.62202969e-04]\n",
      " [2.55737822e-04]\n",
      " [6.18543305e-04]\n",
      " [7.32991328e-04]\n",
      " [2.87828087e-04]\n",
      " [1.14953682e-03]\n",
      " [3.59655219e-04]\n",
      " [1.93507145e-04]\n",
      " [9.49630258e-04]\n",
      " [9.19823185e-04]\n",
      " [3.98072896e-04]\n",
      " [3.93834095e-04]\n",
      " [4.58946511e-04]\n",
      " [7.65745791e-04]\n",
      " [1.40626215e-03]\n",
      " [2.75338774e-04]\n",
      " [1.17703899e-03]\n",
      " [3.09264152e-04]\n",
      " [5.38553791e-04]\n",
      " [6.04188669e-04]\n",
      " [8.88365242e-04]\n",
      " [1.42337916e-03]\n",
      " [3.87409944e-04]\n",
      " [2.56441933e-04]\n",
      " [1.21194867e-04]\n",
      " [1.63141417e-04]\n",
      " [3.39861728e-04]\n",
      " [6.36991111e-04]\n",
      " [1.12050304e-04]\n",
      " [4.49340963e-04]\n",
      " [4.30554886e-04]\n",
      " [4.72059951e-04]\n",
      " [5.98226854e-04]\n",
      " [2.07964325e-04]\n",
      " [4.65985840e-04]\n",
      " [5.24681403e-04]\n",
      " [7.18411696e-04]\n",
      " [2.32248847e-04]\n",
      " [9.00139869e-04]\n",
      " [1.25628801e-03]\n",
      " [3.33934824e-04]\n",
      " [2.25746481e-03]\n",
      " [4.67497377e-04]\n",
      " [1.82006706e-04]\n",
      " [1.84643923e-03]\n",
      " [3.44532696e-04]\n",
      " [9.27241272e-04]\n",
      " [1.02715849e-03]\n",
      " [1.36218545e-03]\n",
      " [1.95922411e-03]\n",
      " [6.84541043e-04]\n",
      " [1.76622969e-04]\n",
      " [1.29151170e-04]\n",
      " [1.57943425e-04]\n",
      " [3.13969497e-04]\n",
      " [5.19190342e-04]\n",
      " [1.28547071e-04]\n",
      " [8.12569879e-04]\n",
      " [7.71962867e-04]\n",
      " [7.75787769e-04]\n",
      " [8.76575862e-04]\n",
      " [3.82502000e-04]\n",
      " [8.01998840e-04]\n",
      " [8.37760389e-04]\n",
      " [1.00860244e-03]\n",
      " [4.19017183e-04]\n",
      " [1.22552406e-03]\n",
      " [1.53697718e-03]\n",
      " [5.42627709e-04]\n",
      " [2.46765098e-03]\n",
      " [6.96120834e-04]\n",
      " [3.40308450e-04]\n",
      " [2.11835962e-04]\n",
      " [6.81587747e-05]\n",
      " [1.04807214e-04]\n",
      " [2.34372927e-04]\n",
      " [4.33382209e-04]\n",
      " [7.50274757e-05]\n",
      " [9.59083034e-05]\n",
      " [9.61741917e-05]\n",
      " [1.37953829e-04]\n",
      " [1.99566171e-04]\n",
      " [5.42663692e-05]\n",
      " [1.21134259e-04]\n",
      " [1.72529392e-04]\n",
      " [2.73248788e-04]\n",
      " [6.76544914e-05]\n",
      " [3.98004815e-04]\n",
      " [5.94183100e-04]\n",
      " [1.31917604e-04]\n",
      " [1.07765809e-03]\n",
      " [2.04461628e-04]\n",
      " [6.44656408e-05]\n",
      " [8.60901216e-04]\n",
      " [7.92556084e-04]\n",
      " [7.31452773e-04]\n",
      " [7.28691657e-04]\n",
      " [3.56648701e-04]\n",
      " [7.43272841e-04]\n",
      " [6.90675270e-04]\n",
      " [7.04142879e-04]\n",
      " [3.38502371e-04]\n",
      " [7.26403083e-04]\n",
      " [7.85134878e-04]\n",
      " [3.35607892e-04]\n",
      " [1.08094168e-03]\n",
      " [3.43507029e-04]\n",
      " [2.04961843e-04]\n",
      " [7.41196180e-04]\n",
      " [7.09490056e-04]\n",
      " [7.75405754e-04]\n",
      " [3.48405771e-04]\n",
      " [7.92751807e-04]\n",
      " [8.96769749e-04]\n",
      " [3.62302955e-04]\n",
      " [1.30144184e-03]\n",
      " [3.98634394e-04]\n",
      " [2.21809171e-04]\n",
      " [1.28847544e-03]\n",
      " [1.59068207e-03]\n",
      " [5.14803813e-04]\n",
      " [2.27243302e-03]\n",
      " [6.07167426e-04]\n",
      " [2.80740421e-04]\n",
      " [3.88352647e-03]\n",
      " [9.06039116e-04]\n",
      " [3.42856770e-04]\n",
      " [2.03627968e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(lr.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "40372526-7650-49f9-b4fd-00b3f47e162a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.698056281493534\n",
      "17.088580720789338\n"
     ]
    }
   ],
   "source": [
    "print(lr.error(X_train, y_train))\n",
    "print(lr.error(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3edb1afb-52dd-439f-9d96-317351ec9109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102, 13)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "850b3039-f4e0-47c7-a742-0fd56c4264bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x232e25e4250>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAARKUlEQVR4nO3df2zc9X3H8dcrjtdcyhYHxdDEDg2rUFYU3HjyEFukCZGiAFtGijQLpKFoKkqllgEVykr6B2P5B6S0QCOtSG5hRQJBIwjmp5ahQFUhVQwHZw4sjag6SuJkxKFzYKu7OOl7f9yZ2MHGd/bdfb+fu+dDsr5377vj3vrKfvHN5/v5fr6OCAEA0rMg6wYAAHNDgANAoghwAEgUAQ4AiSLAASBRC+v5ZcuWLYtVq1bV8ysBIHn79u07ERHt59brGuCrVq3SwMBAPb8SAJJn+1fT1RlCAYBEEeAAkCgCHAASRYADQKIIcABIFAEONIqhXdIDa6R72orboV1Zd4Qaq+s0QgA1MrRLev42aXys+Pzk4eJzSerqza4v1BRH4EAj2Lv9bHhPGB8r1tGwCHCgEZw8UlkdDYEABxrBks7K6mgIBDjmpH9wWOvue0UX3/Wi1t33ivoHh7Nuqbmtv1tqLUyttRaKdTQsTmKiYv2Dw9q2+4DGxs9IkoZHx7Rt9wFJ0qbujixba14TJyr3bi8OmyzpLIY3JzAbGgGOiu3Yc+jj8J4wNn5GO/YcIsCz1NVLYDcZhlBQsaOjYxXVAdQGAY6KrWgrVFQHUBuzBrjtRbb/zfa/237b9j+W6ufbftn2O6Xt0tq3izzYumG1Cq0tU2qF1hZt3bA6o46A5lTOEfj/SboqIr4kaa2ka2xfIekuSXsj4hJJe0vP0QQ2dXfo3hsuU0dbQZbU0VbQvTdcxvg3UGeznsSMiJD0P6WnraWfkHS9pCtL9Ucl/UTSt6reIXJpU3cHgQ1krKwxcNsttvdLOi7p5Yh4XdKFEXFMkkrbC2b47BbbA7YHRkZGqtQ2AKCsAI+IMxGxVlKnpMttryn3CyKiLyJ6IqKnvf0T9+QEAMxRRbNQImJUxaGSayS9b3u5JJW2x6vdHABgZuXMQmm33VZ6XJD0ZUk/l/ScpM2lt22W9GyNegQATKOcKzGXS3rUdouKgb8rIl6w/TNJu2x/VdJ7kv66hn0CAM5RziyUIUnd09Q/kLS+Fk0BAGbHWihAg+gfHNaOPYd0dHRMK9oK2rphNVM9GxwBDjQAVohsTqyFAjSAT1shEo2LAAcaACtENicCHGgArBDZnAhwoAGwQmRz4iQm0AAmTlQyC6W5EOBAg2CFyObDEAoAJIoAB4BEEeAAkCgCHAASRYADQKIIcABIFAEOAIkiwAEgUQQ45mZol/TAGumetuJ2aFfWHQFNhysxUbmhXdLzt0njpZXuTh4uPpekrt7s+gKaDEfgqNze7WfDe8L4WLEOoG4IcFTu5JHK6gBqggBH5ZZ0VlYHUBMEOCq3/m6p9ZwbBbQWinUAdUOAo3JdvdLGndKSlZJc3G7cyQlMoM6YhYK56eolsIGMcQQOAIkiwAEgUQQ4ACSKAAeARBHgAJAoAhwAEkWAA0CiCHAASNSsAW57pe1XbR+0/bbt20v1e2wP295f+rmu9u0CACaUcyXmaUl3RsSbtn9f0j7bL5deeyAivlO79gAAM5k1wCPimKRjpccf2T4oqaPWjQEAPl1FY+C2V0nqlvR6qXSr7SHbj9heOsNnttgesD0wMjIyv24BAB8rO8BtnyfpaUl3RMSHkh6S9AVJa1U8Qv/udJ+LiL6I6ImInvb29vl3DOQN9wdFRspajdB2q4rh/XhE7JakiHh/0us/kPRCTToE8oz7gyJD5cxCsaSHJR2MiPsn1ZdPettXJL1V/faAnOP+oMhQOUfg6yTdLOmA7f2l2rcl3WR7raSQ9K6kr9WgPyDfuD8oMlTOLJTXJHmal16qfjtAYpZ0FodNpqsDNcaVmMB8cH9QZIhbqiFZ/YPD2rHnkI6OjmlFW0FbN6zWpu46X6IwcaJy7/bisMmSzmJ4cwITdUCAI0n9g8PatvuAxsbPSJKGR8e0bfcBScomxAlsZIAhFCRpx55DH4f3hLHxM9qx51BGHQH1R4AjSUdHxyqqA42IAEeSVrQVKqoDjYgAR5K2blitQmvLlFqhtUVbN6zOqCNgBjVcaoGTmEjSxInKzGehAJ+mxkstOCLm/R8pV09PTwwMDNTt+wDUXy6md+bFA2tmuNBrpfTN8lcfsb0vInrOrXMEDqBq+geH9doz39eP9aRWfOaEjv5mmR585kZJX2/OEK/xUguMgQOomv0v9mm7+9S54IQWWOpccELb3af9L/Zl3Vo2ZlpSoUpLLRDgAKrmllOPabFPTakt9indcuqxjDrKWI2XWiDAAVTNigUfVFRveF290sadxTFvubjduLNqV+4yBg6gan5b+JwWjx2bvp5BP7lQw6UWOAJHuriVWe4svna7TrcsmlI73bJIi6/lBhe1wBE40sStzPKpq7cYKpNWZ1zI6ow1Q4AjTZ92KzPCIluszjhFLefFE+CJ4SKJEm5lhgTUetljxsATMvHLMDw6ptDZX4b+weGsW6u/Gs+vBaqh1sseE+AJYQ3sSbiVGRJQ62WPCfCEsAb2JDWeXwtUQ62XPWYMPCEr2goaniasm3YNbE6WIee2blg9ZQxcqu6yxxyBJ4Q1sIG0bOru0L03XKaOtoIsqaOtoHtvuIxZKM2INbCB9Gzq7qjZ3ygBnpha/jIASAtDKACQKAIcABLFEAowT1wdi6wQ4MA81PpSaczD0K4pi2qpARfVYggFmAeujs2pidUqTx6WFGdXq2ywJYcJcGAeuDo2pz5ttcoGQoAD81DrS6UxR02yWuWsAW57pe1XbR+0/bbt20v1822/bPud0nZp7dsF8oWrY3OqSVarLOcI/LSkOyPii5KukPQN25dKukvS3oi4RNLe0nOgqdT6UmnMUZOsVjnrLJSIOCbpWOnxR7YPSuqQdL2kK0tve1TSTyR9qyZdAjnG1bE51NWrN979b618c4cuiBM67mU6fNlW/UmDzUKpaBqh7VWSuiW9LunCUrgrIo7ZvmCGz2yRtEWSLrroonk1CwDl6B8c1rY3Pq+x8e99XCu80aJ7Vw431P9syz6Jafs8SU9LuiMiPiz3cxHRFxE9EdHT3t4+lx4BoCLNMr2zrAC33apieD8eEbtL5fdtLy+9vlzS8dq0CACVaZbpneXMQrGkhyUdjIj7J730nKTNpcebJT1b/fYAoHLNMr2znCPwdZJulnSV7f2ln+sk3SfpatvvSLq69BwAMtcs0zvLmYXymiTP8PL66rYDAPPXLDc/YTGrMuVmxbkmWKAHqIZmmN5JgJehf3BYrz3zff1YT2rFZ07o6G+W6cFnbpT09fr+gkws0DOxxsPEAj0SIQ40IdZCKcP+F/u03X3qXHBCCyx1Ljih7e7T/hf76ttIkyzQA6A8BHgZbjn1mBb71JTaYp/SLaceq28jTbJAD4DyEOBlWLHgg4rqNdMkC/QAKA8BXobfFj5XUb1mmmSBHgDlIcDLsPja7TrdsmhK7XTLIi2+ts5jz1290sad0pKVklzcbtzJCUygSTELpRxdvcUdNWn63sKspu919RLYACQR4OUjOAHkDEMoAJAoAhwAEkWAA0CiCHAASBQBDgCJIsABIFEEOAAkigAHgEQR4ACQKAIcABJFgANAoghwAEgUAQ4AiSLAASBRBDgAJIoAB4BEEeAAkCgCHAASRYADQKIIcABIFAEOAIkiwAEgUQQ4ACSKAAeARM0a4LYfsX3c9luTavfYHra9v/RzXW3bBACcq5wj8B9Jumaa+gMRsbb081J12wIAzGbWAI+In0r6dR16AQBUYD5j4LfaHioNsSyd6U22t9gesD0wMjIyj68DAEw21wB/SNIXJK2VdEzSd2d6Y0T0RURPRPS0t7fP8esAAOeaU4BHxPsRcSYififpB5Iur25bAIDZzCnAbS+f9PQrkt6a6b0AgNpYONsbbD8h6UpJy2wfkfQPkq60vVZSSHpX0tdq1yIAYDqzBnhE3DRN+eEa9AIAqABXYgJAoghwAEgUAQ4AiSLAASBRBDgAJIoAB4BEEeAAkCgCHAASRYADQKIIcABIFAEOAIkiwAEgUQQ4ACSKAAeARBHgAJAoAhwAEkWAA0CiCHAASBQBDgCJIsABIFEEOAAkigAHgEQR4ACQKAIcABJFgANAoghwAEgUAQ4AiSLAASBRBDgAJIoAB4BEEeAAkCgCHAASNWuA237E9nHbb02qnW/7ZdvvlLZLa9smAOBc5RyB/0jSNefU7pK0NyIukbS39BwAUEezBnhE/FTSr88pXy/p0dLjRyVtqm5bAIDZzHUM/MKIOCZJpe0FM73R9hbbA7YHRkZG5vh1AIBz1fwkZkT0RURPRPS0t7fX+usAoGnMNcDft71ckkrb49VrCQBQjrkG+HOSNpceb5b0bHXaAQCUq5xphE9I+pmk1baP2P6qpPskXW37HUlXl54DAOpo4WxviIibZnhpfZV7AQBUgCsxASBRBDgAJIoAB4BEEeAAkCgCHAASRYADQKJmnUaYtf7BYe3Yc0hHR8e0oq2grRtWa1N3R9ZtAUDmch3g/YPD2rb7gMbGz0iShkfHtG33AUkixAE0vVwPoezYc+jj8J4wNn5GO/YcyqgjAMiPXAf40dGxiuoA0ExyHeAr2goV1QGgmeQ6wLduWK1Ca8uUWqG1RVs3rM6oIwDIj1yfxJw4UcksFAD4pFwHuFQMcQIbAD4p10MokqShXdIDa6R72orboV1ZdwQAuZDvI/ChXdLzt0njpVknJw8Xn0tSV292fQFADuT7CHzv9rPhPWF8rFgHgCaX7wA/eaSyOgA0kXwH+JLOyuoA0ETyHeDr75Zaz7lop7VQrANAk8t3gHf1Sht3SktWSnJxu3EnJzABQHmfhSIVw5rABoBPyPcROABgRgQ4ACSKAAeARBHgAJAoAhwAEuWIqN+X2SOSfjXHjy+TdKKK7aSO/XEW+2Iq9sdUjbA/Ph8R7ecW6xrg82F7ICJ6su4jL9gfZ7EvpmJ/TNXI+4MhFABIFAEOAIlKKcD7sm4gZ9gfZ7EvpmJ/TNWw+yOZMXAAwFQpHYEDACYhwAEgUUkEuO1rbB+y/Qvbd2XdT1Zsr7T9qu2Dtt+2fXvWPeWB7Rbbg7ZfyLqXrNlus/2U7Z+Xfk/+NOuesmL7m6W/k7dsP2F7UdY9VVvuA9x2i6R/knStpEsl3WT70my7ysxpSXdGxBclXSHpG028Lya7XdLBrJvIie9J+peI+CNJX1KT7hfbHZJuk9QTEWsktUi6Mduuqi/3AS7pckm/iIhfRsQpSU9Kuj7jnjIREcci4s3S449U/OPsyLarbNnulPQXkn6YdS9Zs/0Hkv5c0sOSFBGnImI006aytVBSwfZCSYslHc24n6pLIcA7JB2e9PyImjy0JMn2Kkndkl7PuJWsPSjp7yX9LuM+8uAPJY1I+ufSkNIPbX8266ayEBHDkr4j6T1JxySdjIh/zbar6kshwD1NrannPto+T9LTku6IiA+z7icrtv9S0vGI2Jd1LzmxUNIfS3ooIrol/a+kpjxnZHupiv9Sv1jSCkmftf032XZVfSkE+BFJKyc971QD/lOoXLZbVQzvxyNid9b9ZGydpL+y/a6KQ2tX2X4s25YydUTSkYiY+FfZUyoGejP6sqT/jIiRiBiXtFvSn2XcU9WlEOBvSLrE9sW2f0/FExHPZdxTJmxbxfHNgxFxf9b9ZC0itkVEZ0SsUvH34pWIaLijrHJFxH9JOmx7dam0XtJ/ZNhSlt6TdIXtxaW/m/VqwBO6ub+pcUSctn2rpD0qnkl+JCLezritrKyTdLOkA7b3l2rfjoiXsmsJOfN3kh4vHez8UtLfZtxPJiLiddtPSXpTxdlbg2rAS+q5lB4AEpXCEAoAYBoEOAAkigAHgEQR4ACQKAIcABJFgANAoghwAEjU/wOdiDlbNc+6hAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(lr.predict(X_test[0:10:]).shape[0]), lr.predict(X_test)[0:10:])\n",
    "plt.scatter(range(y_test[0:10:].shape[0]), y_test[0:10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b189c26d-3edd-438c-981b-2d152c5325ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LinearRegression' object has no attribute 'MSE'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5812/3966131367.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m400000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSE\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m400000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epochs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MSE'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LinearRegression' object has no attribute 'MSE'"
     ]
    }
   ],
   "source": [
    "plt.scatter(np.arange(len(lr.MSE)-400000), lr.MSE[400000:], marker='.')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8701f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        self.w = None   # The (d+1) x 1 numpy array weight matrix\n",
    "        self.degree = 1\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, CF = True, lam = 0, eta = 0.01, epochs = 1000, degree = 1):\n",
    "        ''' Find the fitting weight vector and save it in self.w. \n",
    "            \n",
    "            parameters: \n",
    "                X: n x d matrix of samples, n samples, each has d features, excluding the bias feature\n",
    "                y: n x 1 matrix of lables\n",
    "                CF: True - use the closed-form method. False - use the gradient descent based method\n",
    "                lam: the ridge regression parameter for regularization\n",
    "                eta: the learning rate used in gradient descent\n",
    "                epochs: the maximum epochs used in gradient descent\n",
    "                degree: the degree of the Z-space\n",
    "        '''\n",
    "        self.degree = degree\n",
    "        X = MyUtils.z_transform(X, degree = self.degree)\n",
    "        if CF:\n",
    "            # Set degree = 3 \n",
    "            # Set lam = 0.02211\n",
    "            self._fit_cf(X, y, lam)\n",
    "        else: \n",
    "            self._fit_gd(X, y, lam, eta, epochs)\n",
    " \n",
    "\n",
    "\n",
    "            \n",
    "    def _fit_cf(self, X, y, lam = 0):\n",
    "        ''' Compute the weight vector using the closed-form method.\n",
    "            Save the result in self.w\n",
    "        \n",
    "            X: n x d matrix, n samples, each has d features, excluding the bias feature\n",
    "            y: n x 1 matrix of labels. Each element is the label of each sample. \n",
    "        '''\n",
    "        \n",
    "        # add bias column \n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        # attain the X transpose\n",
    "        X_T = np.transpose(X)\n",
    "        \n",
    "        n, d = np.shape(X)\n",
    "        \n",
    "        # Create the Identity Matrx: \n",
    "        I = np.identity(d)\n",
    "       \n",
    "        # Create (X^T * X)^-1\n",
    "        a = (np.linalg.pinv(X_T @ X + lam * I))\n",
    "        # Create X_T * y \n",
    "        b = X_T @ y \n",
    "\n",
    "        # Set w_star to create w_star: [X_T * y] * [X_T * y]\n",
    "      \n",
    "        self.w = a @ b\n",
    "        \n",
    "\n",
    "        ## delete the `pass` statement below.\n",
    "        ## enter your code here that implements the closed-form method for\n",
    "        ## linear regression \n",
    "        \n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "    # X --> The set used for training\n",
    "    # y --> The correct values in the training set \n",
    "    # lamda --> The reguralizaiton to make it so it is neither bias nor too variant\n",
    "    # epochs --> The number of times it jumps\n",
    "    def _fit_gd(self, X, y, lam = 0, eta = 0.01, epochs = 1000):\n",
    "        ''' Compute the weight vector using the gradient desecent based method.\n",
    "            Save the result in self.w\n",
    "\n",
    "            X: n x d matrix, n samples, each has d features, excluding the bias feature\n",
    "            y: n x 1 matrix of labels. Each element is the label of each sample. \n",
    "        '''\n",
    "\n",
    "        ## enter your code here that implements the gradient descent based method\n",
    "        ## for linear regression \n",
    "        \n",
    "        ### Fix Up X ###\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Add Bias Column \n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        \n",
    "        # Create the Transposed X: X_T\n",
    "        X_T = np.transpose(X)\n",
    "        \n",
    "        n, d = np.shape(X)\n",
    "        \n",
    "        \n",
    "        ### Initialize Variables ### \n",
    "        \n",
    "        # Initialize w to a (dx1)x1 vector\n",
    "        \n",
    "        \n",
    "        # Initialize a: I - (2nu / N) * (X_T @ X)\n",
    "        \n",
    "        # Initialize the identity matrix \n",
    "        I = np.identity(d)\n",
    "        #a = I - (((2 * eta) / n) * (X_T @ X ))\n",
    "        a = I - (((2 * eta) / n) * (X_T @ X + (lam * I)))\n",
    "        \n",
    "        # Initialize b: (2nu/N)*X_T @ y\n",
    "        b = (2 * eta / n) * (X_T @ y)\n",
    "        \n",
    "        ### Run For Loop ###\n",
    "        for i in range(epochs):\n",
    "            # Update w\n",
    "            w_star = a @ w_star + b\n",
    "            \n",
    "        self.w = w_star\n",
    "  \n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ''' parameter:\n",
    "                X: n x d matrix, the n samples, each has d features, excluding the bias feature\n",
    "            return:\n",
    "                n x 1 matrix, each matrix element is the regression value of each sample\n",
    "        '''\n",
    "        X = MyUtils.z_transform(X, degree = self.degree)\n",
    "        \n",
    "        # This takes X and inserts a 0 columnn (the bias) in the front\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "        return X @ self.w\n",
    "        #for x in X:\n",
    "            #out[i] = x @ self.w\n",
    "           # i+= 1\n",
    " \n",
    "        #return out\n",
    "\n",
    "  \n",
    "        \n",
    "        ## enter your code here that produces the label vector for the given samples saved\n",
    "        ## in the matrix X. Make sure your predication is calculated at the same Z\n",
    "        ## space where you trained your model. \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    # The error method\n",
    "    def error(self, X, y):\n",
    "        ''' parameters:\n",
    "                X: n x d matrix of future samples\n",
    "                y: n x 1 matrix of labels\n",
    "            return: \n",
    "                the MSE for this test set (X,y) using the trained model\n",
    "        '''\n",
    "        # Transform to the proper z space\n",
    "        X = MyUtils.z_transform(X, degree = self.degree)\n",
    "        \n",
    "        # Add on a 0 column at the front\n",
    "        X = np.insert(X, [0], 1, axis=1)\n",
    "        \n",
    "        # Get the shape of X\n",
    "        n,d = np.shape(X)\n",
    "        \n",
    "        # Tranpose X\n",
    "        X_T = np.transpose(X)\n",
    "        \n",
    "        # dot together x and w \n",
    "        xw = X @ self.w \n",
    "        \n",
    "        \n",
    "        # Apply and later return the error function\n",
    "        error = np.sum((xw - y)** 2) / n\n",
    "        \n",
    "        return error\n",
    "        ## enter your code here that calculates the MSE between your predicted\n",
    "        ## label vector and the given label vector y, for the sample set saved in matraix x\n",
    "        ## Make sure your predication is calculated at the same Z space where you trained your model. \n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9ceec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyUtils:\n",
    "    def normalize_0_1(X):\n",
    "        ''' Normalize the value of every feature into the [0,1] range, using formula: x = (x-x_min)/(x_max - x_min)\n",
    "            1) First shift all feature values to be non-negative by subtracting the min of each column \n",
    "               if that min is negative.\n",
    "            2) Then divide each feature value by the max of the column if that max is not zero. \n",
    "            \n",
    "            X: n x d matrix of samples, excluding the x_0 = 1 feature. X can have negative numbers.\n",
    "            return: the n x d matrix of samples where each feature value belongs to [0,1]\n",
    "        '''\n",
    "\n",
    "        n, d = X.shape\n",
    "        X_norm = X.astype('float64') # Have a copy of the data in float\n",
    "\n",
    "        for i in range(d):\n",
    "            col_min = min(X_norm[:,i])\n",
    "            col_max = max(X_norm[:,i])\n",
    "            gap = col_max - col_min\n",
    "            if gap:\n",
    "                X_norm[:,i] = (X_norm[:,i] - col_min) / gap\n",
    "            else:\n",
    "                X_norm[:,i] = 0 #X_norm[:,i] - X_norm[:,i]\n",
    "        \n",
    "        return X_norm\n",
    "    def normalize_neg1_pos1(X):\n",
    "        ''' Normalize the value of every feature into the [-1,+1] range. \n",
    "            \n",
    "            X: n x d matrix of samples, excluding the x_0 = 1 feature. X can have negative numbers.\n",
    "            return: the n x d matrix of samples where each feature value belongs to [-1,1]\n",
    "        '''\n",
    "\n",
    "        n, d = X.shape\n",
    "        X_norm = X.astype('float64') # Have a copy of the data in float\n",
    "\n",
    "        for i in range(d):\n",
    "            col_min = min(X_norm[:,i])\n",
    "            col_max = max(X_norm[:,i])\n",
    "            col_mid = (col_max + col_min) / 2\n",
    "            gap = (col_max - col_min) / 2\n",
    "            if gap:\n",
    "                X_norm[:,i] = (X_norm[:,i] - col_mid) / gap\n",
    "            else: \n",
    "                X_norm[:,i] = 0 #X_norm[:,i] - X_norm[:,i]\n",
    "\n",
    "        return X_norm\n",
    "\n",
    "    \n",
    "    def z_transform(X, degree = 2):\n",
    "        \n",
    "        ''' Transforming traing samples to the Z space\n",
    "            X: n x d matrix of samples, excluding the x_0 = 1 feature\n",
    "            degree: the degree of the Z space\n",
    "            return: the n x d' matrix of samples in the Z space, excluding the z_0 = 1 feature.\n",
    "            It can be mathematically calculated: d' = \\sum_{k=1}^{degree} (k+d-1) \\choose (d-1)\n",
    "\n",
    "        '''\n",
    " \n",
    "        # Set r to degree\n",
    "        r = degree\n",
    "        \n",
    "        # degree $leq$ 1, return x \n",
    "        if r <= 1:\n",
    "            return X\n",
    "        \n",
    "        # n is the number of X's rows --> The number of points\n",
    "        # d is the number of X's cols --> The dimensionality \n",
    "        n,d = np.shape(X)\n",
    "        \n",
    "        # Z is going to be a copy of x = Starts out exactly the same \n",
    "        Z = X.copy()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # next it is necessary to create all of the buckets\n",
    "        # a bucket is a matrix with all the possible combinations of multiplications which acheives a certain, single degree \n",
    "        # the # of buckets is conceptuall known d -r -1 Choose d - 1 \n",
    "        # let's save those numbers in an array \n",
    "        \n",
    "        #there will b r buckets \n",
    "        \n",
    "        # B is a list with a bunch of buckets  \n",
    "        B = []\n",
    "        \n",
    "        \n",
    "        # the number of buckets \n",
    "        for i in range(r):\n",
    "            # append a number - the ith bucket size which can be calculated w/ this equation\n",
    "            # math.comb = n choose k \n",
    "            m = d+i # 0-based indexing t.f. the -1 is gone, d is the size of the X matrix \n",
    "            k = d-1 \n",
    "            B.append(math.comb(m,k))\n",
    "    \n",
    "   \n",
    "        ell = np.arange(np.sum(B)) # The summation of all the elements in the B array\n",
    "\n",
    "        q = 0 # the total size of all of the buckets before the previous bucket\n",
    "        \n",
    "        p = d # the size of the previous bucket\n",
    "        g = p\n",
    "        \n",
    "        # at the beginning, there is one bucket \n",
    "        for i in range(1, r): # 1, 2, 3, ... r-1 \n",
    "            \n",
    "            # create each bucket up to the ith bucket, visit the previous bucket \n",
    "            #print(\"New I Loop\\ni: \", i)\n",
    "            # go through every element in the previous bucket - the range starting from q going to q+p \n",
    "            for j in range(q, p):\n",
    "                head = ell[j]\n",
    "\n",
    "        \n",
    "                # this tracks the index of the new column\n",
    "           \n",
    "            \n",
    "                # go from head to lexographically highest feature\n",
    "                for k in range(head, d):\n",
    "\n",
    "                    #elementwise multiplication\n",
    "                    temp = (Z[: ,j] * X[:, k]).reshape(-1,1)\n",
    "                    # insert new column temp on right side\n",
    "                    Z = np.append(Z, temp, axis=1)\n",
    "                    \n",
    "                    # j is hte index of the column you are currently computing\n",
    "                    ell[g] = k # just multiplied w/ x's k column\n",
    "\n",
    "                    g += 1\n",
    "\n",
    "            # adding previous bucket into p the new previous buck\n",
    "            q = p \n",
    "\n",
    "            # the new previous bucket is going to be i which is the current i but will soon be updated \n",
    "            p += B[i] \n",
    " \n",
    "\n",
    "        \n",
    "        assert Z.shape[1] == np.sum(B)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24e12ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Create a dataframe with these columns: \n",
    "# Row ID \n",
    "# Gradient Descent or Closed Form\n",
    "# Z-Degree\n",
    "# Lamda\n",
    "# Eta\n",
    "# Epochs\n",
    "\n",
    "# Test-Set Error\n",
    "# Validation-Set Erro\n",
    "# Over/underfitting/Quality\n",
    "# Description \n",
    "\n",
    "#create the vectors \n",
    "df = pd.DataFrame(columns=('ID', 'isCF', 'Z-Degree', 'Lam', 'ETA', 'Epochs', 'Test_Set Error', 'Validation_Set Error'))\n",
    "dic = {'ID': 0, 'isCF': True, 'Z-Degree': 2, 'Lam': .0001, 'ETA': .05, 'Epochs': 100000, 'Test_Set Error': 5.321, 'Validation_Set Error': 9.123}\n",
    "df = df.append(dic, ignore_index=True)\n",
    "df.to_csv(\"C://Users/Jordan/test-folder/school/years/ML/test.csv\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4e8fb569",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jordan\\AppData\\Local\\Temp/ipykernel_12336/3506432298.py:113: RuntimeWarning: overflow encountered in matmul\n",
      "  w_star = a @ w_star + b\n",
      "C:\\Users\\Jordan\\AppData\\Local\\Temp/ipykernel_12336/3506432298.py:113: RuntimeWarning: invalid value encountered in matmul\n",
      "  w_star = a @ w_star + b\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#create the vectors \n",
    "df = pd.DataFrame(columns=('ID', 'isCF', 'Z-Degree', 'Lam', 'ETA', 'Epochs', 'Test_Set Error', 'Validation_Set Error'))\n",
    "dic = {'ID': 0, 'isCF': True, 'Z-Degree': 2, 'Lam': .0001, 'ETA': .05, 'Epochs': 100000, 'Test_Set Error': 5.321, 'Validation_Set Error': 9.123}\n",
    "df= df.append(dic, ignore_index=True)\n",
    "\n",
    "# Return a dictionary with the parameters \n",
    "\n",
    "i = 0 \n",
    "# Is it running I or GD\n",
    "for b in [True, False]:\n",
    "    # The z-space that it searches\n",
    "    for z in [2, 3]:\n",
    "        lam = 0\n",
    "        # The Lamda that it tries\n",
    "        while lam < .1:\n",
    "            if(not b):\n",
    "                eta = 0\n",
    "                while eta < .1:\n",
    "                    epochs = 50000\n",
    "                    while epochs < 100000:\n",
    "     \n",
    "                        lr.fit(X_train, y_train, CF = False, lam = lam, eta = eta, epochs = epochs, degree = z)\n",
    "                        result = {'ID': i, 'isCF': b, 'Z-Degree': z, 'Lam': lam, 'ETA': eta, 'Epochs': epochs, 'Test_Set Error': lr.error(X_train, y_train), 'Validation_Set Error': lr.error(X_test, y_test)}\n",
    "                        df = df.append(result, ignore_index = True)\n",
    "                        epochs += 100000\n",
    "                \n",
    "                    eta += .01\n",
    "                        \n",
    "            else: \n",
    "                lr.fit(X_train, y_train, CF = True, lam = lam, eta = -1, epochs = -1, degree = z)\n",
    "                result = {'ID': i, 'isCF': b, 'Z-Degree': z, 'Lam': lam, 'ETA': 0, 'Epochs': 0, 'Test_Set Error': lr.error(X_train, y_train), 'Validation_Set Error': lr.error(X_test, y_test)}\n",
    "                df = df.append(result, ignore_index = True)\n",
    "           \n",
    "              \n",
    "             \n",
    "            lam += .1\n",
    "            \n",
    "    i += 1\n",
    "df.to_csv(\"C://Users/Jordan/test-folder/school/years/ML/output1_2.10.21_9.00.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a49e3523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Break\n",
      "    ID  isCF Z-Degree      Lam  Test_Set Error  Validation_Set Error  \\\n",
      "0    0  True        2  0.00010        5.321000              9.123000   \n",
      "1    0  True        3  0.02210        4.122904              9.134032   \n",
      "2    1  True        3  0.02211        4.123236              9.133700   \n",
      "3    2  True        3  0.02212        4.123567              9.133369   \n",
      "4    3  True        3  0.02213        4.123899              9.133038   \n",
      "5    4  True        3  0.02214        4.124230              9.132708   \n",
      "6    5  True        3  0.02215        4.124562              9.132379   \n",
      "7    6  True        3  0.02216        4.124893              9.132050   \n",
      "8    7  True        3  0.02217        4.125224              9.131722   \n",
      "9    8  True        3  0.02218        4.125555              9.131395   \n",
      "10   9  True        3  0.02219        4.125885              9.131068   \n",
      "11  10  True        3  0.02220        4.126216              9.130742   \n",
      "\n",
      "    Total_Error   ETA    Epochs  \n",
      "0           NaN  0.05  100000.0  \n",
      "1     13.256936   NaN       NaN  \n",
      "2     13.256936   NaN       NaN  \n",
      "3     13.256936   NaN       NaN  \n",
      "4     13.256937   NaN       NaN  \n",
      "5     13.256939   NaN       NaN  \n",
      "6     13.256941   NaN       NaN  \n",
      "7     13.256943   NaN       NaN  \n",
      "8     13.256946   NaN       NaN  \n",
      "9     13.256949   NaN       NaN  \n",
      "10    13.256953   NaN       NaN  \n",
      "11    13.256957   NaN       NaN  \n",
      "Break\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#create the vectors \n",
    "df = pd.DataFrame(columns=('ID', 'isCF', 'Z-Degree', 'Lam', 'Test_Set Error', 'Validation_Set Error', 'Total_Error'))\n",
    "df= df.append(dic, ignore_index=True)\n",
    "\n",
    "# Return a dictionary with the parameters \n",
    "\n",
    "i = 0 \n",
    "# Is it running I or GD\n",
    "\n",
    "# The z-space that it searches\n",
    "\n",
    "lam = .0221\n",
    "# The Lamda that it tries\n",
    "while lam < .0222:\n",
    "    lr.fit(X_train, y_train, CF = True, lam = lam, eta = -1, epochs = -1, degree = 3)\n",
    "    train_error = lr.error(X_train, y_train)\n",
    "    val_error = lr.error(X_test, y_test)\n",
    "    tot_error = val_error + train_error\n",
    "    result = {'ID': i, 'isCF': True, 'Z-Degree': 3, 'Lam': lam,'Test_Set Error': train_error, 'Validation_Set Error': val_error, 'Total_Error': tot_error }\n",
    "    df = df.append(result, ignore_index = True) \n",
    "    lam += .00001\n",
    "    \n",
    "        \n",
    "    i += 1\n",
    "print(\"Break\")\n",
    "print(df)\n",
    "print(\"Break\")\n",
    "#df.to_csv(\"C://Users/Jordan/test-folder/school/years/ML/cf_2.11.21_1.15.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4a585a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error:  4.323842057078746\n",
      "Test Error:   9.145022629056024\n",
      "Total Error:  13.46886468613477\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "# Best: lr.fit(X_train, y_train, CF = False, lam = 0, eta = 0.005, epochs = 100000, degree = z_r)\n",
    "lr.fit(X_train, y_train, CF = False, lam = 0.004525, eta = 0.0516, epochs = 105100, degree = 3)\n",
    "train_err = lr.error(X_train, y_train)\n",
    "test_err = lr.error(X_test, y_test)\n",
    "tot_err = train_err + test_err \n",
    "\n",
    "print(\"Train Error: \", train_err)\n",
    "print(\"Test Error:  \", test_err)\n",
    "print(\"Total Error: \", tot_err)\n",
    "#Lam: 0.004525\n",
    "#Eta: 0.05100\n",
    "#Degree: 3 \n",
    "#Epochs: 10516\n",
    "#13.46886468613477\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "24b471ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error:  5.997615303355979\n",
      "Test Error:   12.283182292610926\n",
      "Total Error:  18.280797595966906\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "# Best: lr.fit(X_train, y_train, CF = False, lam = 0, eta = 0.005, epochs = 100000, degree = z_r)\n",
    "lr.fit(X_train, y_train, CF = True, lam = 0.2, eta = 0.0001, epochs = 105100, degree = 3)\n",
    "train_err = lr.error(X_train, y_train)\n",
    "test_err = lr.error(X_test, y_test)\n",
    "tot_err = train_err + test_err \n",
    "\n",
    "print(\"Train Error: \", train_err)\n",
    "print(\"Test Error:  \", test_err)\n",
    "print(\"Total Error: \", tot_err)\n",
    "#Lam: 0.004525\n",
    "#Eta: 0.05100\n",
    "#Degree: 3 \n",
    "#Epochs: 10516\n",
    "#13.46886468613477\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f272aa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n",
      "Please note that for Gradient descent, 0 initialization of the weights is assumed.\n",
      "Please note that for Gradient descent, 0 initialization of the weights is assumed.\n"
     ]
    }
   ],
   "source": [
    "from unittest import loader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from numpy.random import rand as rand\n",
    "from numpy.random import seed as seed\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    (X_train,y_train,X_test,y_test) = loadData()\n",
    "    #testCF(X_train,y_train,X_test,y_test)\n",
    "    testGD(X_train,y_train,X_test,y_test)\n",
    "\n",
    "def testGD(X_train,y_train,X_test,y_test):\n",
    "    errors = loadErrors('GD_Error.npz')\n",
    "    threshold = 1\n",
    "    row = 0\n",
    "\n",
    "    for lam in [0,10]:\n",
    "        for z_r in [1,2]:\n",
    "            for eta in [0.01,0.001]:\n",
    "                lr = LinearRegression() #Create a new lr object each time. No assumption made that the weights will reset.\n",
    "                lr.fit(X_train, y_train, CF = False, lam = lam, eta = eta, epochs = 10000, degree = z_r)\n",
    "                train_error = lr.error(X_train, y_train)\n",
    "                test_error = lr.error(X_test, y_test)\n",
    "\n",
    "                (mikes_train_error,mikes_test_error) = errors[row]\n",
    "                row+=1\n",
    "                if abs(mikes_test_error - test_error) > threshold or abs(mikes_train_error - train_error) > threshold:\n",
    "                    print(f'For GD, and the following params:\\nlam: {lam}, z_r: {z_r}, eta: {eta}')\n",
    "                    print(f'Expected train/test error:\\n{mikes_train_error}, {mikes_test_error}')\n",
    "                    print(f'Found train/test error:\\n{train_error}, {test_error}\\n')\n",
    "    print(\"Please note that for Gradient descent, 0 initialization of the weights is assumed.\")\n",
    "\n",
    "def testCF(X_train,y_train,X_test,y_test):\n",
    "    errors = loadErrors('CF_Error.npz')\n",
    "    threshold = 1\n",
    "    row = 0\n",
    "    print(\"Test\")\n",
    "    for lam in [0,0.1]:\n",
    "        print(\"Test\")\n",
    "        for z_r in [1,2,4]:\n",
    "            \n",
    "            lr = LinearRegression() #Create a new lr object each time. No assumption made that the weights will reset.\n",
    "            lr.fit(X_train, y_train, CF = True, lam = lam, eta = 0.01, epochs = 1000, degree = z_r)\n",
    "            train_error = lr.error(X_train, y_train)\n",
    "            test_error = lr.error(X_test, y_test)\n",
    "            print(\"Test\")\n",
    "            (mikes_train_error,mikes_test_error) = errors[row]\n",
    "            row+=1\n",
    "            if abs(mikes_test_error - test_error) > threshold or abs(mikes_train_error - train_error) > threshold:\n",
    "                print(f'For CF, and the following params:\\nlam: {lam}, z_r: {z_r}')\n",
    "                print(f'Expected train/test error:\\n{mikes_train_error}, {mikes_test_error}')\n",
    "                print(f'Found train/test error:\\n{train_error}, {test_error}\\n')\n",
    "            print(\"Success\")\n",
    "\n",
    "def loadErrors(file):\n",
    "    container = np.load(file)\n",
    "    data = [container[key] for key in container]\n",
    "    errors = np.array(data)\n",
    "    return errors\n",
    "\n",
    "def loadData():\n",
    "    #Reads the files into pandas dataframes from the respective .csv files.\n",
    "    df_X_train = pd.read_csv('houseprice/x_train.csv', header=None)\n",
    "    df_y_train = pd.read_csv('houseprice/y_train.csv', header=None)\n",
    "    df_X_test = pd.read_csv('houseprice/x_test.csv', header=None)\n",
    "    df_y_test = pd.read_csv('houseprice/y_test.csv', header=None)\n",
    "\n",
    "    #Convert the input data into numpy arrays and normalize.\n",
    "    X_train = df_X_train.to_numpy()\n",
    "    X_test = df_X_test.to_numpy()\n",
    "    n_train = X_train.shape[0]\n",
    "\n",
    "    X_all = MyUtils.normalize_0_1(np.concatenate((X_train, X_test), axis=0))\n",
    "    X_train = X_all[:n_train]\n",
    "    X_test = X_all[n_train:]\n",
    "\n",
    "    y_train = df_y_train.to_numpy()\n",
    "    y_test = df_y_test.to_numpy()\n",
    "\n",
    "    #Insure that the data correctly loaded in.\n",
    "    assert X_train.shape == (404, 13), \"Incorrect input, expected (404, 13), found \" + X_train.shape\n",
    "    assert y_train.shape == (404,1), \"Incorrect input, expected (404, 1), found \" + y_train.shape\n",
    "    assert X_test.shape  == (102,13), \"Incorrect input, expected (102, 13), found \" + X_test.shape\n",
    "    assert y_test.shape  == (102,1), \"Incorrect input, expected (102, 1), found \" + y_test.shape\n",
    "\n",
    "    return (X_train,y_train,X_test,y_test)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Here\")\n",
    "    main()\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e4ccefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error:  14.157928627842745\n",
      "Test Error:   23.762675222889214\n",
      "Total Error:  37.92060385073196\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "# Best: lr.fit(X_train, y_train, CF = False, lam = 0, eta = 0.005, epochs = 100000, degree = z_r)\n",
    "lr.fit(X_train, y_train, CF = False, lam = 0, eta = 0.001, epochs = 10000, degree = 3)\n",
    "train_err = lr.error(X_train, y_train)\n",
    "test_err = lr.error(X_test, y_test)\n",
    "tot_err = train_err + test_err \n",
    "\n",
    "print(\"Train Error: \", train_err)\n",
    "print(\"Test Error:  \", test_err)\n",
    "print(\"Total Error: \", tot_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "98b15137",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jordan\\AppData\\Local\\Temp/ipykernel_12336/3506432298.py:113: RuntimeWarning: overflow encountered in matmul\n",
      "  w_star = a @ w_star + b\n",
      "C:\\Users\\Jordan\\AppData\\Local\\Temp/ipykernel_12336/3506432298.py:113: RuntimeWarning: invalid value encountered in matmul\n",
      "  w_star = a @ w_star + b\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "lr = LinearRegression()\n",
    "degree = 4\n",
    "lam = 0\n",
    "x_coordinates = []\n",
    "train_coordinates = []\n",
    "test_coordinates = []\n",
    "tot_coordinates = []\n",
    "while lam < .025:\n",
    "    #lr.fit(X_train, y_train, CF = False, lam =  lam, eta = .01, epochs = 100000, degree = degree)\n",
    "    lr.fit(X_train, y_train, CF = False, lam = 0.004525, eta = 0.0516, epochs = 10000, degree = 4)\n",
    "    train_err = lr.error(X_train, y_train)\n",
    "    test_err = lr.error(X_test, y_test)\n",
    "    tot_err = train_err + test_err\n",
    "    if(train_err < 100 and test_err < 100):\n",
    "        x_coordinates.append(lam)\n",
    "        train_coordinates.append(train_err)\n",
    "        test_coordinates.append(test_err)\n",
    "        tot_coordinates.append(tot_err)\n",
    "        \n",
    "        \n",
    "    lam += .025\n",
    "    \n",
    "\n",
    "txt = \"Gradient Descent (Degree = {degree:.0f})\"\n",
    "plt.title(txt.format(degree = degree))\n",
    "plt.xlabel(\"Error\")\n",
    "plt.ylabel(\"Lambda\")\n",
    "plt.scatter(x_coordinates, train_coordinates, label=\"train err\")\n",
    "plt.scatter(x_coordinates, test_coordinates, label=\"val err\")\n",
    "plt.scatter(x_coordinates, tot_coordinates, label=\"tot err\")\n",
    "leg = plt.legend(loc='upper right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e10e943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
