{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad20f53c-6ac8-477a-9e4e-ff6dd98ecc24",
   "metadata": {},
   "source": [
    "# Testing the logistic regression using various real-world data sets.\n",
    "\n",
    "## Author: Bojian Xu, bojianxu@ewu.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9459c47e-4bca-4f45-be65-136edbd88632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "class MyUtils:\n",
    "    def normalize_0_1(X):\n",
    "        ''' Normalize the value of every feature into the [0,1] range, using formula: x = (x-x_min)/(x_max - x_min)\n",
    "            1) First shift all feature values to be non-negative by subtracting the min of each column \n",
    "               if that min is negative.\n",
    "            2) Then divide each feature value by the max of the column if that max is not zero. \n",
    "            \n",
    "            X: n x d matrix of samples, excluding the x_0 = 1 feature. X can have negative numbers.\n",
    "            return: the n x d matrix of samples where each feature value belongs to [0,1]\n",
    "        '''\n",
    "\n",
    "        n, d = X.shape\n",
    "        X_norm = X.astype('float64') # Have a copy of the data in float\n",
    "\n",
    "        for i in range(d):\n",
    "            col_min = min(X_norm[:,i])\n",
    "            col_max = max(X_norm[:,i])\n",
    "            gap = col_max - col_min\n",
    "            if gap:\n",
    "                X_norm[:,i] = (X_norm[:,i] - col_min) / gap\n",
    "            else:\n",
    "                X_norm[:,i] = 0 #X_norm[:,i] - X_norm[:,i]\n",
    "        \n",
    "        return X_norm\n",
    "    def normalize_neg1_pos1(X):\n",
    "        ''' Normalize the value of every feature into the [-1,+1] range. \n",
    "            \n",
    "            X: n x d matrix of samples, excluding the x_0 = 1 feature. X can have negative numbers.\n",
    "            return: the n x d matrix of samples where each feature value belongs to [-1,1]\n",
    "        '''\n",
    "\n",
    "        n, d = X.shape\n",
    "        X_norm = X.astype('float64') # Have a copy of the data in float\n",
    "\n",
    "        for i in range(d):\n",
    "            col_min = min(X_norm[:,i])\n",
    "            col_max = max(X_norm[:,i])\n",
    "            col_mid = (col_max + col_min) / 2\n",
    "            gap = (col_max - col_min) / 2\n",
    "            if gap:\n",
    "                X_norm[:,i] = (X_norm[:,i] - col_mid) / gap\n",
    "            else: \n",
    "                X_norm[:,i] = 0 #X_norm[:,i] - X_norm[:,i]\n",
    "\n",
    "        return X_norm\n",
    "\n",
    "    \n",
    "    def z_transform(X, degree = 2):\n",
    "        \n",
    "        ''' Transforming traing samples to the Z space\n",
    "            X: n x d matrix of samples, excluding the x_0 = 1 feature\n",
    "            degree: the degree of the Z space\n",
    "            return: the n x d' matrix of samples in the Z space, excluding the z_0 = 1 feature.\n",
    "            It can be mathematically calculated: d' = \\sum_{k=1}^{degree} (k+d-1) \\choose (d-1)\n",
    "\n",
    "        '''\n",
    " \n",
    "        # Set r to degree\n",
    "        r = degree\n",
    "        \n",
    "        # degree $leq$ 1, return x \n",
    "        if r <= 1:\n",
    "            return X\n",
    "        \n",
    "        # n is the number of X's rows --> The number of points\n",
    "        # d is the number of X's cols --> The dimensionality \n",
    "        n,d = np.shape(X)\n",
    "        \n",
    "        # Z is going to be a copy of x = Starts out exactly the same \n",
    "        Z = X.copy()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # next it is necessary to create all of the buckets\n",
    "        # a bucket is a matrix with all the possible combinations of multiplications which acheives a certain, single degree \n",
    "        # the # of buckets is conceptuall known d -r -1 Choose d - 1 \n",
    "        # let's save those numbers in an array \n",
    "        \n",
    "        #there will b r buckets \n",
    "        \n",
    "        # B is a list with a bunch of buckets  \n",
    "        B = []\n",
    "        \n",
    "        \n",
    "        # the number of buckets \n",
    "        for i in range(r):\n",
    "            # append a number - the ith bucket size which can be calculated w/ this equation\n",
    "            # math.comb = n choose k \n",
    "            m = d+i # 0-based indexing t.f. the -1 is gone, d is the size of the X matrix \n",
    "            k = d-1 \n",
    "            B.append(math.comb(m,k))\n",
    "    \n",
    "   \n",
    "        ell = np.arange(np.sum(B)) # The summation of all the elements in the B array\n",
    "\n",
    "        q = 0 # the total size of all of the buckets before the previous bucket\n",
    "        \n",
    "        p = d # the size of the previous bucket\n",
    "        g = p\n",
    "        \n",
    "        # at the beginning, there is one bucket \n",
    "        for i in range(1, r): # 1, 2, 3, ... r-1 \n",
    "            \n",
    "            # create each bucket up to the ith bucket, visit the previous bucket \n",
    "            #print(\"New I Loop\\ni: \", i)\n",
    "            # go through every element in the previous bucket - the range starting from q going to q+p \n",
    "            for j in range(q, p):\n",
    "                head = ell[j]\n",
    "\n",
    "        \n",
    "                # this tracks the index of the new column\n",
    "           \n",
    "            \n",
    "                # go from head to lexographically highest feature\n",
    "                for k in range(head, d):\n",
    "\n",
    "                    #elementwise multiplication\n",
    "                    temp = (Z[: ,j] * X[:, k]).reshape(-1,1)\n",
    "                    # insert new column temp on right side\n",
    "                    Z = np.append(Z, temp, axis=1)\n",
    "                    \n",
    "                    # j is hte index of the column you are currently computing\n",
    "                    ell[g] = k # just multiplied w/ x's k column\n",
    "\n",
    "                    g += 1\n",
    "\n",
    "            # adding previous bucket into p the new previous buck\n",
    "            q = p \n",
    "\n",
    "            # the new previous bucket is going to be i which is the current i but will soon be updated \n",
    "            p += B[i] \n",
    " \n",
    "\n",
    "        \n",
    "        assert Z.shape[1] == np.sum(B)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a7b8aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## >>>>>> Jordan Driscoll 905812\n",
    "\n",
    "# Implementation of the logistic regression with L2 regularization and supports stachastic gradient descent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "\n",
    "\n",
    "# This initializes the Logistic Regression class \n",
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.degree = 1\n",
    "\n",
    "        \n",
    "    def _vectorized_sigmoid(s):\n",
    "        '''\n",
    "            vectorized sigmoid function\n",
    "            \n",
    "            s: n x 1 matrix. Each element is real number represents a signal. \n",
    "            return: n x 1 matrix. Each element is the sigmoid function value of the corresponding signal. \n",
    "        '''\n",
    "        # takes the _sigmoid function and vectorizes it \n",
    "        vmoid = np.vectorize(LogisticRegression._sigmoid)\n",
    "        #outputs a vectorized object \n",
    "        s_out = vmoid(s)\n",
    "   \n",
    "        \n",
    "        return s_out\n",
    "            \n",
    "        # Hint: use the np.vectorize API\n",
    "\n",
    "        \n",
    "    def _sigmoid(s):\n",
    "        ''' s: a real number\n",
    "            return: the sigmoid function value of the input signal s\n",
    "        '''\n",
    "        #return 1 / (1 + e^-s)\n",
    "\n",
    "        \n",
    "        #e = math.e ** -s\n",
    "        e = np.exp(-s)\n",
    "        return (1 / (1 + e))\n",
    "        \n",
    "    # X --> Bias column is pre-inserted\n",
    "    def _Batch_Gradient_Descent(X, y, lam, eta, iterations):\n",
    "        #n - rows\n",
    "        # d- cols \n",
    "        n, d = np.shape(X)\n",
    "        \n",
    "        # Initialize w \n",
    "        w = np.zeros((d,1))\n",
    "        \n",
    "        # the constant which is being multiplied unto the final result to average the result \n",
    "        c = eta / n\n",
    "        \n",
    "        # the reguralization constant \n",
    "        r = 1 - (2 * lam * c)\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            \n",
    "            s = y * (X @ w)\n",
    "            \n",
    "            vs = LogisticRegression._vectorized_sigmoid(-s)\n",
    "    \n",
    "            # does the w equation \n",
    "            w = r * w  + (c * (X.T @ (y * vs)))\n",
    "\n",
    "        return w\n",
    "    \n",
    "    # Code for teh Stochastic Gradient Descent\n",
    "    def _Stochastic_Gradient_Descent(X, y, lam, eta, iterations, mini_batch_size):\n",
    "        # should it get shuffled? \n",
    "        \n",
    "        shuffle = True\n",
    "        # n - rows\n",
    "        # d - cols \n",
    "        n, d = np.shape(X)\n",
    "        \n",
    "        # Create boundries for the mini-batch-size\n",
    "        if(mini_batch_size > n or mini_batch_size < 1):\n",
    "            mini_batch_size = n\n",
    "        \n",
    "        m = mini_batch_size\n",
    "        \n",
    "        \n",
    "        if(shuffle):\n",
    "            # Makes it such that X & y are shuffled with each other \n",
    "            Xy = np.append(X, y, axis=1)\n",
    "            # shuffles the Xy array \n",
    "            np.random.shuffle(Xy)\n",
    "                \n",
    "            # extracts y \n",
    "            y = Xy[:, d:]\n",
    "                \n",
    "            # extracts X \n",
    "            X = Xy[:, :d]\n",
    "            \n",
    "        \n",
    "        # Initialize w \n",
    "        w = np.zeros((d,1))\n",
    "        \n",
    "        # the constant which is being multiplied unto the final result to average the result \n",
    "        c = eta / m\n",
    "        \n",
    "        # the reguralization constant \n",
    "        r = 1 - (2 * lam * c)\n",
    "        \n",
    "        curr_start = 0 \n",
    "        curr_end = curr_start + m \n",
    "        start = curr_start \n",
    "        for i in range(iterations):\n",
    "\n",
    "            #Create the miniature versions of the grpah \n",
    "            X_mini = X[start:curr_end , :]\n",
    "            y_mini = y[start:curr_end , :]\n",
    "            \n",
    "            s = y_mini * (X_mini @ w)\n",
    "            \n",
    "            vs = LogisticRegression._vectorized_sigmoid(-s)\n",
    "              \n",
    "            \n",
    "            \n",
    "            # does the w equation \n",
    "            w = (c * ((y_mini * vs).T @ X_mini)).T + (r * w)\n",
    "            \n",
    "            if(n != m):\n",
    "                    \n",
    "                # update the current starting location\n",
    "                curr_start += m\n",
    "                # update end \n",
    "                curr_end = curr_start+m\n",
    "                # set s to c \n",
    "                start = curr_start\n",
    "        \n",
    "                # if e's too big but c isn't \n",
    "                if(curr_end >= n and curr_start < n):\n",
    "                    start = curr_start\n",
    "                    curr_start = -m\n",
    "                    curr_end = n\n",
    "        \n",
    "                # if e is too big \n",
    "                if(curr_end > n):\n",
    "                    curr_start = 0\n",
    "                    curr_end = curr_start + m\n",
    "    \n",
    "            \n",
    "           \n",
    "        \n",
    "        # Sets the w to the result \n",
    "        return w \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, lam = 0, eta = 0.01, iterations = 1000, SGD = False, mini_batch_size = 10, degree = 1):\n",
    "        ''' Save the passed-in degree of the Z space in `self.degree`. \n",
    "            Compute the fitting weight vector and save it `in self.w`. \n",
    "         \n",
    "            Parameters: \n",
    "                X: n x d matrix of samples; every sample has d features, excluding the bias feature. \n",
    "                y: n x 1 vector of lables. Every label is +1 or -1. \n",
    "                lam: the L2 parameter for regularization\n",
    "                eta: the learning rate used in gradient descent\n",
    "                iterations: the number of iterations used in GD/SGD. Each iteration is one epoch if batch GD is used. \n",
    "                SGD: True - use SGD; False: use batch GD\n",
    "                mini_batch_size: the size of each mini batch size, if SGD is True.  \n",
    "                degree: the degree of the Z space\n",
    "        '''\n",
    "        \n",
    "        # Initialize the class-level degree to the inputted degree\n",
    "        self.degree = degree\n",
    "    \n",
    "        # First, fix up X to put it into the correct degree\n",
    "        X = MyUtils.z_transform(X, degree = self.degree)\n",
    "\n",
    "        # Then add the bias column onto X \n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        \n",
    "        if (not SGD):\n",
    "            self.w = LogisticRegression._Batch_Gradient_Descent(X, y, lam, eta, iterations) \n",
    "        else: \n",
    "            self.w = LogisticRegression._Stochastic_Gradient_Descent(X, y, lam, eta, iterations, mini_batch_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "   \n",
    "    \n",
    "    def predict(self, X):\n",
    "        ''' parameters:\n",
    "                X: n x d matrix; n samples; each has d features, excluding the bias feature. \n",
    "            return: \n",
    "                n x 1 matrix: each row is the probability of each sample being positive. \n",
    "        '''\n",
    "        # First, fix up X to put it into the correct degree\n",
    "        X = MyUtils.z_transform(X, degree = self.degree)\n",
    "\n",
    "        # Then add the bias column onto X \n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        \n",
    "        return LogisticRegression._vectorized_sigmoid(X @ self.w)\n",
    "    \n",
    "    \n",
    "    def error(self, X, y):\n",
    "        ''' parameters:\n",
    "                X: n x d matrix; n samples; each has d features, excluding the bias feature. \n",
    "                y: n x 1 matrix; each row is a labels of +1 or -1.\n",
    "            return:\n",
    "                The number of misclassified samples. \n",
    "                Every sample whose sigmoid value > 0.5 is given a +1 label; otherwise, a -1 label.\n",
    "        '''\n",
    "        err = self.predict(X)\n",
    "        error_total = 0 \n",
    "        # Goes through each prediction\n",
    "        for e1, y_i in zip(err, y):\n",
    "            # Gives the error a label \n",
    "            e1_label = 1 if (e1 > .5) else -1 \n",
    "            \n",
    "            if(e1_label != y_i):\n",
    "                error_total += 1  \n",
    "        \n",
    "        # Find every \n",
    "        \n",
    "        return error_total\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b86e4e9c-1bbe-4955-9b48-a71a2a4d8a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ionosphere/hello\n"
     ]
    }
   ],
   "source": [
    "data_set = 'ionosphere'\n",
    "\n",
    "print(data_set+'/'+'hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b122b78-4cb1-4e42-a725-1427857ba84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280, 34)\n",
      "(280, 1)\n",
      "(71, 34)\n",
      "(71, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# READ in data\n",
    "df_X_train = pd.read_csv(data_set+'/'+'X_train.csv', header=None)\n",
    "df_y_train = pd.read_csv(data_set+'/'+'y_train.csv', header=None)\n",
    "df_X_test = pd.read_csv(data_set+'/'+'X_test.csv', header=None)\n",
    "df_y_test = pd.read_csv(data_set+'/'+'y_test.csv', header=None)\n",
    "\n",
    "# save in numpy arrays\n",
    "X_train = df_X_train.to_numpy()\n",
    "y_train = df_y_train.to_numpy()\n",
    "X_test = df_X_test.to_numpy()\n",
    "y_test = df_y_test.to_numpy()\n",
    "\n",
    "# get training set size\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "# normalize all features to [0,1] or [-1,1]\n",
    "if data_set == 'ionosphere':\n",
    "    X_all = MyUtils.normalize_neg1_pos1(np.concatenate((X_train, X_test), axis=0))\n",
    "\n",
    "\n",
    "X_train = X_all[:n_train]\n",
    "X_test = X_all[n_train:]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "#print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b963de09-dd31-4382-8f97-4aceb4df56d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [-1.]\n",
      " [ 1.]\n",
      " [-1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8927c9-c95b-419c-8c66-a0bf831e5bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49faf0ea-5e92-4939-9549-871faf8c96f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "log = LogisticRegression()\n",
    "\n",
    "# train the model\n",
    "#log.fit(X_train, y_train, lam = 0, eta = 0.1, iterations = 5000, SGD = False, mini_batch_size = 1, degree = 1)\n",
    "\n",
    "\n",
    "log.fit(X_train, y_train, lam = 0, eta = 0.1, iterations = 5000, SGD = True, mini_batch_size = 100, degree = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8ecd81e-7afb-4465-933a-98d53465e419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "misclassfied percentage from training:  0.06428571428571428\n",
      "misclassfied percentage from validation:  0.14084507042253522\n"
     ]
    }
   ],
   "source": [
    "print('misclassfied percentage from training: ', log.error(X_train, y_train)/X_train.shape[0])\n",
    "print('misclassfied percentage from validation: ', log.error(X_test, y_test)/X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa662a0-5c7e-4399-be73-478fdec87dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = log.predict(X_test)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a159f2c-5b6c-457a-a2fb-24878a34b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y_test.shape[0]):\n",
    "    print('test sample ', i)\n",
    "    if np.sign(preds[i]-0.5) != y_test[i]:\n",
    "        print('misclassified!!')\n",
    "    print('predicted probablity of being +1 is: ', preds[i])\n",
    "    print('label is', y_test[i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f409ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3c6769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb1b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f7cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.exp(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b145dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sign([1, -2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9cd48499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed GD\n",
      "Success: \n",
      "For SGD, and the following params:\n",
      "lam: 0, z_r: 1, eta: 0.01\n",
      "Success: \n",
      "For SGD, and the following params:\n",
      "lam: 0, z_r: 1, eta: 0.01\n",
      "Success: \n",
      "For SGD, and the following params:\n",
      "lam: 0, z_r: 1, eta: 0.01\n",
      "Success: \n",
      "For SGD, and the following params:\n",
      "lam: 0, z_r: 1, eta: 0.001\n",
      "Success: \n",
      "For SGD, and the following params:\n",
      "lam: 0, z_r: 1, eta: 0.001\n",
      "Success: \n",
      "For SGD, and the following params:\n",
      "lam: 0, z_r: 1, eta: 0.001\n",
      "Success: \n",
      "For SGD, and the following params:\n",
      "lam: 0, z_r: 2, eta: 0.01\n",
      "Success: \n",
      "For SGD, and the following params:\n",
      "lam: 0, z_r: 2, eta: 0.01\n",
      "Success: \n",
      "For SGD, and the following params:\n",
      "lam: 0, z_r: 2, eta: 0.01\n",
      "Success: \n",
      "For SGD, and the following params:\n",
      "lam: 0, z_r: 2, eta: 0.001\n",
      "Success: \n",
      "For SGD, and the following params:\n",
      "lam: 0, z_r: 2, eta: 0.001\n",
      "Success: \n",
      "For SGD, and the following params:\n",
      "lam: 0, z_r: 2, eta: 0.001\n",
      "Success: \n",
      "For SGD, and the following params:\n",
      "lam: 1, z_r: 1, eta: 0.01\n",
      "Success: \n",
      "For SGD, and the following params:\n",
      "lam: 1, z_r: 1, eta: 0.01\n",
      "Success: \n",
      "For SGD, and the following params:\n",
      "lam: 1, z_r: 1, eta: 0.01\n",
      "Success: \n",
      "For SGD, and the following params:\n",
      "lam: 1, z_r: 1, eta: 0.001\n",
      "Success: \n",
      "For SGD, and the following params:\n",
      "lam: 1, z_r: 1, eta: 0.001\n",
      "Success: \n",
      "For SGD, and the following params:\n",
      "lam: 1, z_r: 1, eta: 0.001\n",
      "Success: \n",
      "For SGD, and the following params:\n",
      "lam: 1, z_r: 2, eta: 0.01\n",
      "Success: \n",
      "For SGD, and the following params:\n",
      "lam: 1, z_r: 2, eta: 0.01\n",
      "Success: \n",
      "For SGD, and the following params:\n",
      "lam: 1, z_r: 2, eta: 0.01\n",
      "Success: \n",
      "For SGD, and the following params:\n",
      "lam: 1, z_r: 2, eta: 0.001\n",
      "Success: \n",
      "For SGD, and the following params:\n",
      "lam: 1, z_r: 2, eta: 0.001\n",
      "Success: \n",
      "For SGD, and the following params:\n",
      "lam: 1, z_r: 2, eta: 0.001\n",
      "SUCCESSFUL RUN!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import code_logistic_regression.logistic_regression as logic\n",
    "import sys\n",
    "#from code_misc.utils import MyUtils\n",
    "\n",
    "\n",
    "def main():\n",
    "    (X_train,y_train,X_test,y_test) = loadData()\n",
    "    passedGD = passedSGD = False\n",
    "    testSigmoid()\n",
    "\n",
    "    passedGD = True#testGD(X_train,y_train,X_test,y_test)\n",
    "    print(\"Passed GD\")\n",
    "    passedSGD = testSGD(X_train,y_train,X_test,y_test)\n",
    "    if passedGD and passedSGD:\n",
    "        print(\"SUCCESSFUL RUN!\")\n",
    "    else:\n",
    "        print(f'PassedGD: {passedGD}, PassedSGD: {passedSGD}')\n",
    "        print(\"Non-shuffling of the data is assumed as well for the tester's data.\")\n",
    "\n",
    "\n",
    "def testGD(X_train,y_train,X_test,y_test):\n",
    "    errors = loadErrors('GD_Error.npz')\n",
    "    row = 0\n",
    "    passed = True\n",
    "\n",
    "    for lam in [0,10]:\n",
    "        for z_r in [1,2]:\n",
    "            for eta in [0.01,0.001]:\n",
    "                log = LogisticRegression() #Create a new lr object each time. No assumption made that the weights will reset.\n",
    "                log.fit(X_train, y_train, lam = lam, eta = eta, iterations = 10000, SGD = False, mini_batch_size = 20, degree = z_r)\n",
    "                train_error = log.error(X_train, y_train)\n",
    "                test_error = log.error(X_test, y_test)\n",
    "\n",
    "                (mikes_train_error,mikes_test_error) = errors[row]\n",
    "                row+=1\n",
    "                if not inThreshold(train_error,mikes_train_error) or not inThreshold(test_error,mikes_test_error):\n",
    "                    print(f'For GD, and the following params:\\nlam: {lam}, z_r: {z_r}, eta: {eta}')\n",
    "                    print(f'Expected train/test error:\\n{mikes_train_error}, {mikes_test_error}')\n",
    "                    print(f'Found train/test error:\\n{train_error}, {test_error}\\n')\n",
    "                    passed = False\n",
    "                else:\n",
    "                    print(\"Success: \")\n",
    "                    print(f'For GD, and the following params:\\nlam: {lam}, z_r: {z_r}, eta: {eta}')\n",
    "    if not passed:\n",
    "        print(\"Please note that for Gradient descent, 0 initialization of the weights is assumed.\")\n",
    "        print(\"Due to randomness resulting from shuffling the data, minor differences can be safely ignored.\")\n",
    "    return passed\n",
    "\n",
    "def testSGD(X_train,y_train,X_test,y_test):\n",
    "    errors = loadErrors('SGD_Error.npz')\n",
    "    row = 0\n",
    "    passed = True\n",
    "    \n",
    "    n,d = X_train.shape\n",
    "    for lam in [0,1]:\n",
    "        for z_r in [1,2]:\n",
    "            for eta in [0.01,0.001]:\n",
    "                for mbs in [1,20,n]:\n",
    "                    log = LogisticRegression() #Create a new log object each time. No assumption made that the weights will reset.\n",
    "                    log.fit(X_train, y_train, lam = lam, eta = eta, iterations = 10000, SGD = True, mini_batch_size = mbs, degree = z_r)\n",
    "                    train_error = log.error(X_train, y_train)\n",
    "                    test_error = log.error(X_test, y_test)\n",
    "\n",
    "                    (mikes_train_error,mikes_test_error) = errors[row]\n",
    "                    row+=1\n",
    "                    if not inThreshold(train_error,mikes_train_error) or not inThreshold(test_error,mikes_test_error):\n",
    "                        print(f'For SGD, and the following params:\\nlam: {lam}, z_r: {z_r}, eta: {eta}')\n",
    "                        print(f'Expected train/test error:\\n{mikes_train_error}, {mikes_test_error}')\n",
    "                        print(f'Found train/test error:\\n{train_error}, {test_error}\\n')\n",
    "                        passed = False\n",
    "                    else:\n",
    "                        print(\"Success: \")\n",
    "                        print(f'For SGD, and the following params:\\nlam: {lam}, z_r: {z_r}, eta: {eta}')\n",
    "                        \n",
    "    if not passed:\n",
    "        print(\"Please note that for Stochastic Gradient descent, 0 initialization of the weights is assumed.\")\n",
    "        print(\"Due to randomness resulting from shuffling the data, minor differences can be safely ignored.\")\n",
    "    return passed\n",
    "\n",
    "\n",
    "def testSigmoid():\n",
    "    actualValue = [0.5,0.7310586,0.8807971,0.2689414]\n",
    "    i = 0\n",
    "    for s in [0,1,2,-1]:\n",
    "        assert inThreshold(LogisticRegression._sigmoid(s),actualValue[i],0.00001), f\"Incorrect sigmoid value, expected {actualValue[i]}, but found {LogisticRegression._sigmoid(s)} for s = {s}\"\n",
    "        i += 1\n",
    "\n",
    "def inThreshold(x1, x2, threshold=1.1):\n",
    "    return abs(x1 - x2) < threshold\n",
    "\n",
    "def loadData(data_set='ionoshpere'):\n",
    "    #Reads the files into pandas dataframes from the respective .csv files.\n",
    "    path = './ionosphere'\n",
    "    df_X_train = pd.read_csv(f'{path}/X_train.csv', header=None)\n",
    "    df_y_train = pd.read_csv(f'{path}/y_train.csv', header=None)\n",
    "    df_X_test = pd.read_csv(f'{path}/X_test.csv', header=None)\n",
    "    df_y_test = pd.read_csv(f'{path}/y_test.csv', header=None)\n",
    "\n",
    "    #Convert the input data into numpy arrays and normalize.\n",
    "    X_train = df_X_train.to_numpy()\n",
    "    X_test = df_X_test.to_numpy()\n",
    "    n_train = X_train.shape[0]\n",
    "\n",
    "    X_all = MyUtils.normalize_neg1_pos1(np.concatenate((X_train, X_test), axis=0))\n",
    "    X_train = X_all[:n_train]\n",
    "    X_test = X_all[n_train:]\n",
    "\n",
    "    y_train = df_y_train.to_numpy()\n",
    "    y_test = df_y_test.to_numpy()\n",
    "\n",
    "    #Insure that the data correctly loaded in.\n",
    "    assert X_train.shape == (280, 34), \"Incorrect input, expected (280, 34), found \" + X_train.shape\n",
    "    assert y_train.shape == (280, 1), \"Incorrect input, expected (280, 1), found \" + y_train.shape\n",
    "    assert X_test.shape  == (71, 34), \"Incorrect input, expected (71, 34), found \" + X_test.shape\n",
    "    assert y_test.shape  == (71, 1), \"Incorrect input, expected (71, 1), found \" + y_test.shape\n",
    "\n",
    "    return (X_train,y_train,X_test,y_test)\n",
    "\n",
    "def loadErrors(file):\n",
    "    container = np.load(file)\n",
    "    data = [container[key] for key in container]\n",
    "    errors = np.array(data)\n",
    "    return errors\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0914c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "a = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\n",
    "b = np.array([[1],[2],[3]])\n",
    "a_, b_ = LogisticRegression._create_minibatch(a, b, 3)\n",
    "print(a_ == a)\n",
    "print(b_ == b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "93b4b506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "Output\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "Output\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "Output\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "Output\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "Output\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "Output\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "Output\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "Output\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "Output\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# the total size of the array \n",
    "n = 15\n",
    "\n",
    "# the current starting interval \n",
    "c = 0 \n",
    "\n",
    "# the size of the mini_batch\n",
    "m = 5\n",
    "\n",
    "# the index of the end of the mini_batch\n",
    "e = c + m \n",
    "\n",
    "# what is being cut \n",
    "s = c\n",
    "\n",
    "# how many times it loops through \n",
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    print(\"Output\")\n",
    "    # output \n",
    "    for j in range(s, e):\n",
    "        print(j)\n",
    "  \n",
    "    # update c \n",
    "    c += m\n",
    "    # update end \n",
    "    e = c+m\n",
    "    # set s to c \n",
    "    s = c\n",
    "    if(n == m):\n",
    "        s = 0\n",
    "        c = 0 \n",
    "        e = n\n",
    "    else:\n",
    "        # if e's too big but c isn't \n",
    "        if(e >= n and c <= n):\n",
    "            s = c\n",
    "            c = -m\n",
    "            e = n\n",
    "            \n",
    "        # if e is too big \n",
    "        if(e > n):\n",
    "            c = 0\n",
    "            e = c + m\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca7ad65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
