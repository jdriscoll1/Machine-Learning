{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad20f53c-6ac8-477a-9e4e-ff6dd98ecc24",
   "metadata": {},
   "source": [
    "# Testing the dense NN for classification using the MNIST image data\n",
    "\n",
    "## Author: Bojian Xu, bojianxu@ewu.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9459c47e-4bca-4f45-be65-136edbd88632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nn\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "class MyUtils:\n",
    "    def normalize_0_1(X):\n",
    "        ''' Normalize the value of every feature into the [0,1] range, using formula: x = (x-x_min)/(x_max - x_min)\n",
    "            1) First shift all feature values to be non-negative by subtracting the min of each column \n",
    "               if that min is negative.\n",
    "            2) Then divide each feature value by the max of the column if that max is not zero. \n",
    "            \n",
    "            X: n x d matrix of samples, excluding the x_0 = 1 feature. X can have negative numbers.\n",
    "            return: the n x d matrix of samples where each feature value belongs to [0,1]\n",
    "        '''\n",
    "\n",
    "        n, d = X.shape\n",
    "        X_norm = X.astype('float64') # Have a copy of the data in float\n",
    "\n",
    "        for i in range(d):\n",
    "            col_min = min(X_norm[:,i])\n",
    "            col_max = max(X_norm[:,i])\n",
    "            gap = col_max - col_min\n",
    "            if gap:\n",
    "                X_norm[:,i] = (X_norm[:,i] - col_min) / gap\n",
    "            else:\n",
    "                X_norm[:,i] = 0 #X_norm[:,i] - X_norm[:,i]\n",
    "        \n",
    "        return X_norm\n",
    "    def normalize_neg1_pos1(X):\n",
    "        ''' Normalize the value of every feature into the [-1,+1] range. \n",
    "            \n",
    "            X: n x d matrix of samples, excluding the x_0 = 1 feature. X can have negative numbers.\n",
    "            return: the n x d matrix of samples where each feature value belongs to [-1,1]\n",
    "        '''\n",
    "\n",
    "        n, d = X.shape\n",
    "        X_norm = X.astype('float64') # Have a copy of the data in float\n",
    "\n",
    "        for i in range(d):\n",
    "            col_min = min(X_norm[:,i])\n",
    "            col_max = max(X_norm[:,i])\n",
    "            col_mid = (col_max + col_min) / 2\n",
    "            gap = (col_max - col_min) / 2\n",
    "            if gap:\n",
    "                X_norm[:,i] = (X_norm[:,i] - col_mid) / gap\n",
    "            else: \n",
    "                X_norm[:,i] = 0 #X_norm[:,i] - X_norm[:,i]\n",
    "\n",
    "        return X_norm\n",
    "\n",
    "    \n",
    "    def z_transform(X, degree = 2):\n",
    "        \n",
    "        ''' Transforming traing samples to the Z space\n",
    "            X: n x d matrix of samples, excluding the x_0 = 1 feature\n",
    "            degree: the degree of the Z space\n",
    "            return: the n x d' matrix of samples in the Z space, excluding the z_0 = 1 feature.\n",
    "            It can be mathematically calculated: d' = \\sum_{k=1}^{degree} (k+d-1) \\choose (d-1)\n",
    "\n",
    "        '''\n",
    " \n",
    "        # Set r to degree\n",
    "        r = degree\n",
    "        \n",
    "        # degree $leq$ 1, return x \n",
    "        if r <= 1:\n",
    "            return X\n",
    "        \n",
    "        # n is the number of X's rows --> The number of points\n",
    "        # d is the number of X's cols --> The dimensionality \n",
    "        n,d = np.shape(X)\n",
    "        \n",
    "        # Z is going to be a copy of x = Starts out exactly the same \n",
    "        Z = X.copy()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # next it is necessary to create all of the buckets\n",
    "        # a bucket is a matrix with all the possible combinations of multiplications which acheives a certain, single degree \n",
    "        # the # of buckets is conceptuall known d -r -1 Choose d - 1 \n",
    "        # let's save those numbers in an array \n",
    "        \n",
    "        #there will b r buckets \n",
    "        \n",
    "        # B is a list with a bunch of buckets  \n",
    "        B = []\n",
    "        \n",
    "        \n",
    "        # the number of buckets \n",
    "        for i in range(r):\n",
    "            # append a number - the ith bucket size which can be calculated w/ this equation\n",
    "            # math.comb = n choose k \n",
    "            m = d+i # 0-based indexing t.f. the -1 is gone, d is the size of the X matrix \n",
    "            k = d-1 \n",
    "            B.append(math.comb(m,k))\n",
    "    \n",
    "   \n",
    "        ell = np.arange(np.sum(B)) # The summation of all the elements in the B array\n",
    "\n",
    "        q = 0 # the total size of all of the buckets before the previous bucket\n",
    "        \n",
    "        p = d # the size of the previous bucket\n",
    "        g = p\n",
    "        \n",
    "        # at the beginning, there is one bucket \n",
    "        for i in range(1, r): # 1, 2, 3, ... r-1 \n",
    "            \n",
    "            # create each bucket up to the ith bucket, visit the previous bucket \n",
    "\n",
    "            # go through every element in the previous bucket - the range starting from q going to q+p \n",
    "            for j in range(q, p):\n",
    "                head = ell[j]\n",
    "\n",
    "        \n",
    "                # this tracks the index of the new column\n",
    "           \n",
    "            \n",
    "                # go from head to lexographically highest feature\n",
    "                for k in range(head, d):\n",
    "\n",
    "                    #elementwise multiplication\n",
    "                    temp = (Z[: ,j] * X[:, k]).reshape(-1,1)\n",
    "                    # insert new column temp on right side\n",
    "                    Z = np.append(Z, temp, axis=1)\n",
    "                    \n",
    "                    # j is hte index of the column you are currently computing\n",
    "                    ell[g] = k # just multiplied w/ x's k column\n",
    "\n",
    "                    g += 1\n",
    "\n",
    "            # adding previous bucket into p the new previous buck\n",
    "            q = p \n",
    "\n",
    "            # the new previous bucket is going to be i which is the current i but will soon be updated \n",
    "            p += B[i] \n",
    " \n",
    "\n",
    "        \n",
    "        assert Z.shape[1] == np.sum(B)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "004d2501-4984-4e71-97fc-ba307dd1a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10  #number of classes\n",
    "d = 784 #number of features, excluding the bias feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b122b78-4cb1-4e42-a725-1427857ba84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# READ in data\n",
    "df_X_train = pd.read_csv('MNIST/x_train.csv', header=None)\n",
    "df_y_train = pd.read_csv('MNIST/y_train.csv', header=None)\n",
    "df_X_test = pd.read_csv('MNIST/x_test.csv', header=None)\n",
    "df_y_test = pd.read_csv('MNIST/y_test.csv', header=None)\n",
    "\n",
    "# save in numpy arrays\n",
    "X_train_raw = df_X_train.to_numpy()\n",
    "y_train_raw = df_y_train.to_numpy()\n",
    "X_test_raw = df_X_test.to_numpy()\n",
    "y_test_raw = df_y_test.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "# get training set size\n",
    "n_train = X_train_raw.shape[0]\n",
    "n_test = X_test_raw.shape[0]\n",
    "\n",
    "# normalize all features to [0,1]\n",
    "X_all = MyUtils.normalize_0_1(np.concatenate((X_train_raw, X_test_raw), axis=0))\n",
    "X_train = X_all[:n_train]\n",
    "X_test = X_all[n_train:]\n",
    "\n",
    "\n",
    "# convert each label into a 0-1 vector\n",
    "y_train = np.zeros((n_train, k))\n",
    "y_test = np.zeros((n_test, k))\n",
    "for i in range(n_train):\n",
    "    y_train[i,int(y_train_raw[i])] = 1.0\n",
    "for i in range(n_test):\n",
    "    y_test[i,int(y_test_raw[i])] = 1.0\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "#print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b963de09-dd31-4382-8f97-4aceb4df56d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(y_train[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac8927c9-c95b-419c-8c66-a0bf831e5bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the network\n",
    "nuts = NeuralNetwork()\n",
    "\n",
    "nuts.add_layer(d = d)  # input layer - 0\n",
    "\n",
    "nuts.add_layer(d = 100, act = 'relu')  # hidden layer - 1\n",
    "nuts.add_layer(d = 30, act = 'relu')  # hiddent layer - 2\n",
    "#nuts.add_layer(d = 100, act = 'relu')  # hiddent layer - 3\n",
    "#nuts.add_layer(d = 30, act = 'relu')  # hiddent layer - 4\n",
    "\n",
    "nuts.add_layer(d = k, act = 'logis')  # output layer,    multi-class classification, #classes = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "558e4ccf-178b-4d39-9724-e986279d7daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_star:  (20, 10)\n",
      "Y:  (20, 10)\n",
      "X_star:  (20, 10)\n",
      "Y:  (20, 10)\n",
      "X_star:  (20, 10)\n",
      "Y:  (20, 10)\n",
      "X_star:  (20, 10)\n",
      "Y:  (20, 10)\n",
      "X_star:  (20, 10)\n",
      "Y:  (20, 10)\n",
      "X_star:  (20, 10)\n",
      "Y:  (20, 10)\n",
      "X_star:  (20, 10)\n",
      "Y:  (20, 10)\n",
      "X_star:  (20, 10)\n",
      "Y:  (20, 10)\n",
      "X_star:  (20, 10)\n",
      "Y:  (20, 10)\n",
      "X_star:  (20, 10)\n",
      "Y:  (20, 10)\n"
     ]
    }
   ],
   "source": [
    "nuts.fit(X_train, y_train, eta = 0.1, iterations = 10, SGD = True, mini_batch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8ecd81e-7afb-4465-933a-98d53465e419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.83838333]\n",
      "[0.8418]\n"
     ]
    }
   ],
   "source": [
    "print(nuts.error(X_train, y_train))\n",
    "print(nuts.error(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caa662a0-5c7e-4399-be73-478fdec87dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = nuts.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4185626-ad8d-47ca-8253-77eb0fe21a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "#print(preds[:100])\n",
    "#print(y_test_raw[:100])\n",
    "print(np.sum(preds != y_test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a159f2c-5b6c-457a-a2fb-24878a34b4e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27408/2745720946.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my_test_raw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'misclassified!!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predicted as'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'label is'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_raw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "for i in range(y_test.shape[0]):\n",
    "    if preds[i] != y_test_raw[i]:\n",
    "        print('misclassified!!')\n",
    "    print('predicted as', preds[i])\n",
    "    print('label is', y_test_raw[i])\n",
    "    pixels = X_test_raw[i].reshape((28, 28))\n",
    "    plt.imshow(pixels, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53ef467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jordan Driscoll 905812\n",
    "\n",
    "## delete the `pass` statement in every function below and add in your own code. \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Various math functions, including a collection of activation functions used in NN.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MyMath:\n",
    "\n",
    "    def tanh(x):\n",
    "        ''' tanh function. \n",
    "            Support vectorized operation\n",
    "\n",
    "            x: an array type of real numbers\n",
    "            return: the numpy array where every element is tanh of the corresponding element in array x\n",
    "        '''\n",
    "        v_tanh = np.vectorize(np.tanh)\n",
    "        return v_tanh(x)\n",
    "   \n",
    "\n",
    "    \n",
    "    def tanh_de(x):\n",
    "        ''' Derivative of the tanh function. \n",
    "            Support vectorized operation\n",
    "\n",
    "            x: an array type of real numbers\n",
    "            return: the numpy array where every element is tanh derivative of the corresponding element in array x\n",
    "        '''\n",
    "        v_tanh_de = np.vectorize(lambda x: 1 - (np.tanh(x) ** 2))\n",
    "        return v_tanh_de(x)\n",
    "\n",
    "    \n",
    "    \n",
    "    def logis(x):\n",
    "        ''' Logistic function. \n",
    "            Support vectorized operation\n",
    "\n",
    "            x: an array type of real numbers\n",
    "            return: the numpy array where every element is logistic of \n",
    "                    the corresponding element in array x\n",
    "        '''\n",
    "        v_sigmoid = np.vectorize(lambda x: 1 / (1 + np.exp(-x)))\n",
    "        return v_sigmoid(x)\n",
    "         \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    def logis_de(x):\n",
    "        ''' Derivative of the logistic function. \n",
    "            Support vectorized operation\n",
    "\n",
    "            x: an array type of real numbers\n",
    "            return: the numpy array where every element is logistic derivative of \n",
    "                    the corresponding element in array x\n",
    "        '''\n",
    "        # The sigmoid function\n",
    "       \n",
    "        # The vectorized sigmoid function\n",
    "        sigmoid = lambda s: 1 / (1 + np.exp(-s))\n",
    "        \n",
    "        v_sigmoid = np.vectorize(lambda x: sigmoid(x) * (1 - sigmoid(x)))\n",
    "        \n",
    "        return v_sigmoid(x)\n",
    "        \n",
    "\n",
    "    \n",
    "    def iden(x):\n",
    "        ''' Identity function\n",
    "            Support vectorized operation\n",
    "            \n",
    "            x: an array type of real numbers\n",
    "            return: the numpy array where every element is the same as\n",
    "                    the corresponding element in array x\n",
    "        '''\n",
    "        return np.array(x) \n",
    "        \n",
    "\n",
    "    \n",
    "    def iden_de(x):\n",
    "        ''' The derivative of the identity function \n",
    "            Support vectorized operation\n",
    "\n",
    "            x: an array type of real numbers\n",
    "            return: the numpy array of all zeros of the same shape of x.\n",
    "        '''\n",
    "        return np.ones(np.array(x).shape)\n",
    "        \n",
    "\n",
    "    \n",
    "    def relu(x):\n",
    "        ''' The ReLU function \n",
    "            Support vectorized operation\n",
    "\n",
    "            x: an array type of real numbers\n",
    "            return: the numpy array where every element is the max of: zero vs. the corresponding element in x.\n",
    "        '''\n",
    "        v_relu = np.vectorize(lambda x: x * (x > 0))\n",
    "        return v_relu(x)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    def _relu_de_scaler(x):\n",
    "        ''' The derivative of the ReLU function. Scaler version.\n",
    "        \n",
    "            x: a real number\n",
    "            return: 1, if x > 0; 0, otherwise.\n",
    "        '''\n",
    "        return 1 if x > 0 else 0 \n",
    "\n",
    "    \n",
    "    def relu_de(x):\n",
    "        ''' The derivative of the ReLU function \n",
    "            Support vectorized operation\n",
    "\n",
    "            x: an array type of real numbers\n",
    "            return: the numpy array where every element is the _relu_de_scaler of the corresponding element in x.   \n",
    "        '''\n",
    "        v_relu_de = np.vectorize(MyMath._relu_de_scaler)\n",
    "        return v_relu_de(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "536b9c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jordan Driscoll 905812\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Implementation of the forwardfeed neural network using stachastic gradient descent via backpropagation\n",
    "# Support parallel/batch mode: process every (mini)batch as a whole in one forward-feed/backtracking round trip. \n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import math_util as mu\n",
    "import nn_layer\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = []     # the list of L+1 layers, including the input layer. \n",
    "        self.L = -1          # Number of layers, excluding the input layer. \n",
    "                             # Initting it as -1 is to exclude the input layer in L.\n",
    "\n",
    "    \n",
    "    \n",
    "    def add_layer(self, d = 1, act = 'tanh'):\n",
    "        ''' The newly added layer is always added AFTER all existing layers.\n",
    "            The firstly added layer is the input layer.\n",
    "            The most recently added layer is the output layer. \n",
    "            \n",
    "            d: the number of nodes, excluding the bias node, which will always be added by the program. \n",
    "            act: the choice of activation function. The input layer will never use an activation function even if it is given. \n",
    "            \n",
    "            So far, the set of supported activation functions are (new functions can be easily added into `math_util.py`): \n",
    "            - 'tanh': the tanh function\n",
    "            - 'logis': the logistic function\n",
    "            - 'iden': the identity function\n",
    "            - 'relu': the ReLU function\n",
    "        '''\n",
    "        # Create the new neural layer using the passed in variables\n",
    "        new_layer = NeuralLayer(d, act)\n",
    "        # Add the layers to the list of layers\n",
    "        self.layers.append(new_layer)\n",
    "        # Add the layer index\n",
    "        self.L += 1\n",
    "        \n",
    "    \n",
    "\n",
    "    def _init_weights(self):\n",
    "        ''' Initialize every layer's edge weights with random numbers from [-1/sqrt(d),1/sqrt(d)], \n",
    "            where d is the number of nonbias node of the layer\n",
    "        '''\n",
    "        \n",
    "        weight_rng= np.random.default_rng(2142)\n",
    "        # We go through the layers excluding the first one \n",
    "        for layer_id in range(1, self.L + 1): \n",
    "            \n",
    "            curr_layer = self.layers[layer_id]\n",
    "            prev_layer = self.layers[layer_id - 1]\n",
    "           \n",
    "            # go through and create the w for each l \n",
    "            layer_n = prev_layer.d + 1 # number of nodes this layer, +1 is included as bias  \n",
    "            layer_d = curr_layer.d # number of connections this layer\n",
    "            \n",
    "            range_min = -1 / math.sqrt(layer_d)\n",
    "            range_max = 1 / math.sqrt(layer_d)\n",
    "            \n",
    "            curr_layer.W = weight_rng.uniform(low=range_min, high=range_max, size=(layer_n,layer_d))\n",
    "            \n",
    "            # curr_layer.W = np.random.uniform(low=range_min, high=range_max, size=(layer_n,layer_d))\n",
    "            \n",
    "            \n",
    "            # Create a Matrix of Zeros of Size l_n, l_d \n",
    "\n",
    "    \n",
    "    \n",
    "    ############### Update Final Delta and Gradient ############################\n",
    "    def _update_final_gradient_and_delta(self, c, Y):\n",
    "        \n",
    "        final_layer = self.layers[self.L] \n",
    "        \n",
    "        transform_str = \"ij,ik->jk\"\n",
    "        \n",
    "        # Delta^L --> Matrix of all of the vectors of all of the partial derivatives of the errors\n",
    "        # D_L = 2 * (X* - Y) * derrived activation(S)            \n",
    "        # The final X without the bias \n",
    "        X_star = final_layer.X[:, 1:]\n",
    "        \n",
    "        # The final S \n",
    "        S_final = final_layer.S\n",
    "        \n",
    "        # The final Delta\n",
    "        a = 2 * (X_star - Y)\n",
    "        print(\"X_star: \", X_star.shape)\n",
    "        print(\"Y: \", Y.shape)\n",
    "        b = final_layer.act_de(S_final)\n",
    "        \n",
    "        Delta_final = a * b\n",
    "                \n",
    "        final_layer.Delta = Delta_final\n",
    "        \n",
    "        ############### OBATINING G ###########################\n",
    "        # G is the gradient --> This is the line of ups & downs\n",
    "        # Second to last layer's X \n",
    "        A = self.layers[self.L - 1].X\n",
    "        \n",
    "        B = Delta_final\n",
    "        \n",
    "        # Update the final G\n",
    "        final_layer.G = c * np.einsum(transform_str, A, B)\n",
    "\n",
    "    \n",
    "    def _backpropogate(self, layer_id, c):\n",
    "        # The previous layer \n",
    "        p_layer = self.layers[layer_id - 1]\n",
    "        \n",
    "        # The Current Layer\n",
    "        layer = self.layers[layer_id]\n",
    "        \n",
    "        #The next layer\n",
    "        n_layer = self.layers[layer_id + 1]\n",
    "        \n",
    "        # Obtain the S with the derived activation function applied to it \n",
    "        S = layer.act_de(layer.S)\n",
    "        \n",
    "        # The W in the equation w/out bias \n",
    "        W = n_layer.W[1:].T \n",
    "        \n",
    "        # The delta is the next layer's delta\n",
    "        d = n_layer.Delta\n",
    "        \n",
    "        layer.Delta = S * (d @ W)\n",
    "        \n",
    "        # X is the previous layer's X\n",
    "        A = p_layer.X\n",
    "        \n",
    "        B = layer.Delta\n",
    "        \n",
    "        transform_str = \"ij,ik->jk\"\n",
    "        \n",
    "        # Calculate Gradient from each layer \n",
    "        layer.G = c * np.einsum(transform_str, A, B)\n",
    "    \n",
    "    \n",
    "    def _update_minibatch(self, d):\n",
    "        # update c \n",
    "        d['c'] += d['m']\n",
    "        # update end \n",
    "        d['e'] = d['c']+ d['m']\n",
    "        # set s to c \n",
    "        d['s'] = d['c']\n",
    "        if(d['n'] == d['m']):\n",
    "            d['s'] = 0\n",
    "            d['c'] = 0 \n",
    "            d['e'] = d['n']\n",
    "        else:\n",
    "            # if e's too big but c isn't \n",
    "            if(d['e'] >= d['n'] and d['c'] <= d['n']):\n",
    "                d['s'] = d['c']\n",
    "                d['c'] = -d['m']\n",
    "                d['e'] = d['n']\n",
    "                \n",
    "            # if e is too big \n",
    "            # e > n \n",
    "            if(d['e'] > d['n']):\n",
    "                # c = 0\n",
    "                d['c'] = 0\n",
    "                # e = c + m\n",
    "                d['e'] = d['c'] + d['m']\n",
    "        \n",
    "            return d\n",
    "        \n",
    "    # Prediction Should Be (97%)\n",
    "    def fit(self, X, Y, eta = 0.01, iterations = 1000, SGD = True, mini_batch_size = 1):\n",
    "        ''' Find the fitting weight matrices for every hidden layer and the output layer. Save them in the layers.\n",
    "          \n",
    "            X: n x d matrix of samples, where d >= 1 is the number of features in each training sample\n",
    "            Y: n x k vector of lables, where k >= 1 is the number of classes in the multi-class classification\n",
    "            eta: the learning rate used in gradient descent\n",
    "            iterations: the maximum iterations used in gradient descent\n",
    "            SGD: True - use SGD; False: use batch GD\n",
    "            mini_batch_size: the size of each mini batch size, if SGD is True.  \n",
    "        '''\n",
    "        print(\"Initial Y Shape: \", Y.shape)\n",
    "        \n",
    "        self._init_weights()  # initialize the edge weights matrices with random numbers.\n",
    "        \n",
    "        shuffle = True\n",
    "            \n",
    "        # The first layer\n",
    "        first_layer = self.layers[0]\n",
    "        \n",
    "        # Obtain the shape of n \n",
    "        n, d = X.shape\n",
    "        \n",
    "        if(not SGD):\n",
    "            mini_batch_size = n        \n",
    "        \n",
    "        m = mini_batch_size\n",
    "        \n",
    "        # Shuffle X * y \n",
    "        if(shuffle):\n",
    "            # Makes it such that X & y are shuffled with each other \n",
    "            Xy = np.append(X, Y, axis=1)\n",
    "            # shuffles the Xy array \n",
    "            np.random.shuffle(Xy)\n",
    "                \n",
    "            # extracts y \n",
    "            Y = Xy[:, d:]\n",
    "                \n",
    "            # extracts X \n",
    "            X = Xy[:, :d]\n",
    "\n",
    "        \n",
    "        # This stores all of the necessary data for the minibatch\n",
    "        minibatch_data = dict({'n':n, 'c':0, 'm':mini_batch_size,'e':mini_batch_size,'s':0})\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Run for the number of times that are available\n",
    "        for iteration in range(iterations):           \n",
    "            \n",
    "            X_mini = X[minibatch_data['c']:minibatch_data['e']]\n",
    "            y_mini = Y[minibatch_data['c']:minibatch_data['e']]\n",
    "            print(\"Y mini shape: \", y_mini.shape)\n",
    "            c = 1 / X_mini.shape[0]\n",
    "            \n",
    "            # Add Bias Column to the 0th layer's X \n",
    "            first_layer.X = np.insert(X_mini, 0, 1, axis=1)\n",
    "        \n",
    "            \n",
    "            \n",
    "            ######### FORWARD FEEDING #################\n",
    "            # For Each Layer Excluding the First One \n",
    "            for layer_id in range(1, self.L + 1):       \n",
    "                self._Forward_Feed(layer_id)    \n",
    "                \n",
    "     \n",
    "\n",
    "            ############ UPDATE FINAL G & D #################\n",
    "            # Update the final gradient and Delta\n",
    "            self._update_final_gradient_and_delta(c, y_mini)\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            ############## BACKPROPOGATION #######################\n",
    "            # Then go through and update all the delta previous to that\n",
    "            # Back Propogation: Starting from the back going to the front\n",
    "            for layer_id in reversed(range(1, self.L)):\n",
    "                self._backpropogate(layer_id, c)\n",
    "                \n",
    "                \n",
    "            \n",
    "            ############## UPDATE THE WEIGHTS ################\n",
    "            for layer_id in range(1, self.L + 1):\n",
    "                layer = self.layers[layer_id]\n",
    "                layer.W -= eta * layer.G\n",
    "         \n",
    "            # Adjust \n",
    "            if(SGD):\n",
    "                minibatch_data = self._update_minibatch(minibatch_data)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        # I will leave you to decide how you want to organize the rest of the code, but below is what I used and recommend. Decompose them into private components/functions. \n",
    "\n",
    "        ## prep the data: add bias column; randomly shuffle data training set. \n",
    "\n",
    "        ## for every iteration:\n",
    "        #### get a minibatch and use it for:\n",
    "        ######### forward feeding\n",
    "        ######### calculate the error of this batch if you want to track/observe the error trend for viewing purpose.\n",
    "        ######### back propagation to calculate the gradients of all the weights\n",
    "        ######### use the gradients to update all the weight matrices. \n",
    "\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def _Forward_Feed(self, layer_id): \n",
    "        \n",
    "        # the previous layer\n",
    "        p_layer = self.layers[layer_id - 1]\n",
    "            \n",
    "        # the current layer \n",
    "        layer = self.layers[layer_id]\n",
    "        \n",
    "        # Set the S \n",
    "        S = p_layer.X @ layer.W\n",
    "        \n",
    "        # Run the activation function on S\n",
    "        layer.X = layer.act(S)\n",
    "        \n",
    "        # Add bias to the X \n",
    "        layer.X = np.insert(layer.X, 0, 1, axis=1)\n",
    "        \n",
    "        # Set the layer's S \n",
    "        layer.S = S \n",
    "        \n",
    "\n",
    "        \n",
    "    def predict(self, X):\n",
    "        ''' X: n x d matrix, the sample batch, excluding the bias feature 1 column.\n",
    "            \n",
    "            return: n x 1 matrix, n is the number of samples, every row is the predicted class id.\n",
    "         '''\n",
    "        # Get X to the right shape \n",
    "        # Set as an np array\n",
    "        X = np.array(X)\n",
    "        \n",
    "        X = X.reshape(-1, self.layers[0].d)\n",
    "        \n",
    "        # Add bias column\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        \n",
    "        # Take the first layer and make it's inputted X the sample X being predicted\n",
    "        self.layers[0].X = X\n",
    "        \n",
    "        # go through layer excluding the first one \n",
    "        for layer_id in range(1, self.L + 1):\n",
    "                       \n",
    "            self._Forward_Feed(layer_id)\n",
    "            \n",
    "            \n",
    "        \n",
    "        # Take the final layer \n",
    "        final_layer = self.layers[self.L]\n",
    "        \n",
    "        # Knock off the bias column\n",
    "        final_layer.X = final_layer.X[:, 1:]\n",
    "        \n",
    "        # Then return an n x 1 with the arg max's\n",
    "        out = np.argmax(final_layer.X, axis=1)\n",
    "        \n",
    "        out = out.reshape(-1, 1)\n",
    "        \n",
    "        return out \n",
    "         \n",
    "            \n",
    "             \n",
    "            \n",
    "          \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        return _Forward_Feed(X)\n",
    "    \n",
    "        \n",
    "    \n",
    "    def error(self, X, Y):\n",
    "        ''' X: n x d matrix, the sample batch, excluding the bias feature 1 column. \n",
    "               n is the number of samples. \n",
    "               d is the number of (non-bias) features of each sample. \n",
    "            Y: n x k matrix, the labels of the input n samples. Each row is the label of one sample, \n",
    "               where only one entry is 1 and the rest are all 0. \n",
    "               Y[i,j]=1 indicates the ith sample belongs to class j.\n",
    "               k is the number of classes. \n",
    "            \n",
    "            return: the percentage of misclassfied samples\n",
    "            \n",
    "        '''\n",
    "        \n",
    "        n, d = X.shape\n",
    "        \n",
    "        # Find the nx1 predicition matrix \n",
    "        preds = self.predict(X)\n",
    "        \n",
    "        # Take all of the maximum's from the Y matrix - results in an nx1 matrix\n",
    "        y = np.argmax(Y, axis=1) \n",
    "        \n",
    "        y = y.reshape(-1, 1) \n",
    "        \n",
    "        # Return the summation of the number of equal args\n",
    "        err = sum(preds != y)\n",
    "        \n",
    "        # # of err / # of samples\n",
    "        return err / n\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a68204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef5309d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Bojian Xu, bojianxu@ewu.edu\n",
    "\n",
    "\n",
    "# Implementation of one layer used in the forward feeding and back propagation neural network \n",
    "\n",
    "import numpy as np\n",
    "import math_util as mu\n",
    "\n",
    "class NeuralLayer:\n",
    "    def __init__(self, d = 1, act = 'tanh'):\n",
    "        ''' d: the number of NON-bias nodes in the layer\n",
    "                             \n",
    "            act: the activation function. It will not be useful/used, regardlessly, at the input layer.\n",
    "                 1) 'tanh': the tanh function\n",
    "                 2) 'logis': the logistic function\n",
    "                 3) 'iden': the identity function\n",
    "                 4) 'relu': the ReLU function \n",
    "        '''\n",
    "\n",
    "        self.d = d   # the number of non-bias nodes\n",
    "\n",
    "        self.act = eval('MyMath.' + act)   # the activation function, not useful/used at the input layer\n",
    "        self.act_de = eval('MyMath.' + act + '_de')  # the derivative of the activation function, not useful/used at the input layer \n",
    "        \n",
    "        # The following matrix/vectors are to be materalized by the NN-level code. Some are not useful for the input layer. \n",
    "        # Below, N' represents the minibatch size, \\ell represents the index of this layer. \n",
    "        self.S = None       # N' x d^{(\\ell)} matrix. Each row is the vector of the d signals, sent into the d nodes, by each sample. Not useful for the input layer. \n",
    "        self.X = None       # N' x (d^{(\\ell)}+1) matrix. Each row is the vector of the d+1 outputs, sent out by the bias node and the d neurons, by each sample.\n",
    "        self.Delta = None   # N' x d^{(\\ell)} matrix. Each row is vector of delta = \\partial E / \\partial S, where E is the error. Not useful for the input layer\n",
    "        self.G = None       # (d^{(\\ell-1)}+1 ) x d^{(\\ell)} matrix. The gradient of E over W.\n",
    "        self.W = None       # (d^{(\\ell-1)}+1 ) x d^{(\\ell)} matrix. The weights of the edges coming into layer \\ell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b486a03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed Math!\n",
      "Passed Add Layer!\n",
      "Passed Init Weights!\n",
      "Passed Seeded Weights!\n",
      "Here we are!!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "verbose = True\n",
    "\n",
    "def main():\n",
    "    passed_math_util = test_math_util()\n",
    "    print(\"Passed Math!\")\n",
    "    passed_add_layer = test_add_layer()\n",
    "    print(\"Passed Add Layer!\")\n",
    "    passed_init_weights = test_init_weights()\n",
    "    print(\"Passed Init Weights!\")\n",
    "\n",
    "    # Check if weights are initializing to seed's values.\n",
    "    # weight_rng = np.random.default_rng(2142) and weight_rng.uniform used to generate weights.\n",
    "    passed_seeded_weights = test_seeded_weights() #Also, verifies if _init_weights and add_layer is working.\n",
    "  \n",
    "    \n",
    "    if not passed_seeded_weights:\n",
    "        print(\"Additional tests cannot be accurately performed without a set of seeded weights.\\nUnder nn.NeuralNetwork._init_weights, please use weight_rng = np.random.default_rng(2142) and weight_rng.uniform used to generate weights.\")\n",
    "        print(f\"Current results:\\npassed_math_util: {passed_math_util}, passed_add_layer: {passed_add_layer}\\n\\\n",
    "            passed_init_weights{passed_init_weights}, passed_seeded_weights: {passed_seeded_weights}\")\n",
    "        return\n",
    "    print(\"Passed Seeded Weights!\")\n",
    "    (X_train,y_train,X_test,y_test) = loadData()\n",
    "    print(\"Here we are!!\")\n",
    "\n",
    "def test_seeded_weights():\n",
    "    d = 2\n",
    "    k = 2\n",
    "    passed = True\n",
    "\n",
    "    # build the network\n",
    "    nuts = NeuralNetwork()\n",
    "\n",
    "    nuts.add_layer(d = d)  # input layer - 0\n",
    "    nuts.add_layer(d = 5, act = 'relu')  # hidden layer - 1\n",
    "    nuts.add_layer(d = k, act = 'logis')  # output layer\n",
    "\n",
    "    nuts._init_weights()\n",
    "\n",
    "    seed_weights = load_weights()\n",
    "    for layer, seed_weight in zip(nuts.layers[1:],seed_weights):\n",
    "        seed_weight = np.array(seed_weight)\n",
    "        weight = np.array(layer.W)\n",
    "\n",
    "        if (weight != seed_weight).any():\n",
    "            if verbose:\n",
    "                print(f\"check_seeded_weights:\\nExpected:\\n{seed_weight}\\nFound:\\n{weight}\")\n",
    "            passed = False\n",
    "\n",
    "    return passed\n",
    "\n",
    "def test_init_weights():\n",
    "    d = 10\n",
    "    k = 8\n",
    "    passed = True\n",
    "\n",
    "    # build the network\n",
    "    nuts = NeuralNetwork()\n",
    "\n",
    "    nuts.add_layer(d = d)  # input layer - 0\n",
    "    nuts.add_layer(d = 5, act = 'relu')  # hidden layer - 1\n",
    "    nuts.add_layer(d = k, act = 'logis')  # output layer\n",
    "\n",
    "    nuts._init_weights()\n",
    "\n",
    "    #Check dimensionality of weights\n",
    "    if nuts.layers[0].W != None:\n",
    "        print(\"\")\n",
    "\n",
    "    shapes = [(11,5),(6,8)]\n",
    "    for layer, dim in zip(nuts.layers[1:], shapes):\n",
    "        if layer.W.shape != dim:\n",
    "            if verbose:\n",
    "                print(f\"test_init_weights: Invalid dimensions of the instantiated weights. Expected {dim}, found {layer.W.shape}\")\n",
    "            passed = False\n",
    "\n",
    "    return passed\n",
    "\n",
    "def test_add_layer():\n",
    "    # build the network\n",
    "    nuts = NeuralNetwork()\n",
    "\n",
    "    assert nuts.L == -1, f\"After initialization, L = -1. Found L = {nuts.L}\"\n",
    "\n",
    "    passed = True\n",
    "\n",
    "    nuts.add_layer(d = 5, act = 'logis')\n",
    "    if nuts.L != 0:\n",
    "        if verbose:\n",
    "            print(f\"test_add_layer: After adding a layer, L = 0. Found L = {nuts.L}\")\n",
    "        passed = False\n",
    "    if len(nuts.layers) != 1:\n",
    "        if verbose:\n",
    "            print(f\"test_add_layer: Failed to add layer to layers.\")\n",
    "        passed = False\n",
    "\n",
    "    return passed\n",
    "\n",
    "def test_math_util():\n",
    "    passed = [test_tanh(), test_tanh_de(), test_logis(), test_logis_de(), test_iden(), test_iden_de(), test_relu(), test_relu_de()]\n",
    "    if all(passed):\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"test_math_util passed methods:\\n\\\n",
    "tanh: {passed[0]}, tanh_de: {passed[1]}\\n\\\n",
    "logis: {passed[2]}, logis_de: {passed[3]}\\n\\\n",
    "iden: {passed[4]}, iden_de: {passed[5]}\\n\\\n",
    "relu: {passed[6]}, relu_de: {passed[7]}\")\n",
    "    \n",
    "        return False\n",
    "\n",
    "def test_tanh():\n",
    "    x = [0.0,1.0,2.0,-1.0]\n",
    "    y = [0.0,0.761594156,0.9640275801,-0.761594156]\n",
    "    y_hat = MyMath.tanh(x)\n",
    "    return _test_math_util(x,y,y_hat,\"tanh\")\n",
    "\n",
    "def test_tanh_de():\n",
    "    x = [0.0,1.0,2.0,-1.0]\n",
    "    y = [1,0.419974,0.0706508,0.419974]\n",
    "    y_hat = MyMath.tanh_de(x)\n",
    "    return _test_math_util(x,y,y_hat,\"tanh_de\")\n",
    "\n",
    "def test_logis():\n",
    "    x = [0.0,1.0,2.0,-1.0]\n",
    "    y = [0.5,0.7310586,0.8807971,0.2689414]\n",
    "    y_hat = MyMath.logis(x)\n",
    "    return _test_math_util(x,y,y_hat,\"logis\")\n",
    "\n",
    "def test_logis_de():\n",
    "    x = [0.0,1.0,2.0,-1.0]\n",
    "    y = [0.25,0.196612,0.104994,0.196612]\n",
    "    y_hat = MyMath.logis_de(x)\n",
    "    return _test_math_util(x,y,y_hat,\"logis_de\")\n",
    "\n",
    "def test_iden():\n",
    "    x = [0.0,1.0,2.0,-1.0]\n",
    "    y = [0.0,1.0,2.0,-1.0]\n",
    "    y_hat = MyMath.iden(x)\n",
    "    return _test_math_util(x,y,y_hat,\"iden\")\n",
    "\n",
    "def test_iden_de():\n",
    "    x = [0.0,1.0,2.0,-1.0]\n",
    "    y = [1,1,1,1]\n",
    "    y_hat = MyMath.iden_de(x)\n",
    "    return _test_math_util(x,y,y_hat,\"iden_de\")\n",
    "\n",
    "def test_relu():\n",
    "    x = [0.0,1.0,2.0,-1.0]\n",
    "    y = [0.0,1.0,2.0,0.0]\n",
    "    y_hat = MyMath.relu(x)\n",
    "    return _test_math_util(x,y,y_hat,\"relu\")\n",
    "\n",
    "def test_relu_de():\n",
    "    x = [0.0,1.0,2.0,-1.0]\n",
    "    y = [0,1,1,0]\n",
    "    y_hat = MyMath.relu_de(x)\n",
    "    return _test_math_util(x,y,y_hat,\"relu_de\")\n",
    "\n",
    "def _test_math_util(x,y,y_hat,name):\n",
    "\n",
    "    passed = True\n",
    "\n",
    "    if not _is_numpy_array(y_hat):\n",
    "        if verbose:\n",
    "            print(f\"Incorrect return type. Expected type {np.ndarray}, but found {type(y_hat)}\")\n",
    "        passed = False\n",
    "\n",
    "    for y, y_hat, x in zip(y, y_hat, x):\n",
    "        if not inThreshold(y,y_hat,0.00001):\n",
    "            if verbose:\n",
    "                print(f\"Incorrect {name} value, expected {y}, but found {y_hat} for x = {x}\")\n",
    "            passed = False\n",
    "    return passed\n",
    "\n",
    "def _is_numpy_array(x):\n",
    "    return isinstance(x,np.ndarray)\n",
    "\n",
    "def inThreshold(x1, x2, threshold=1.1):\n",
    "    return abs(x1 - x2) < threshold\n",
    "\n",
    "def saveAllZ(Z_list,file=\"output.npz\"):\n",
    "    np.savez(file,*Z_list)\n",
    "\n",
    "def clearZ(file=\"output.npz\"):\n",
    "    f = open(file, \"w\")\n",
    "    f.close()\n",
    "\n",
    "def loadData(data_set='ionoshpere'):\n",
    "    k = 10\n",
    "    d = 784\n",
    "\n",
    "    #Reads the files into pandas dataframes from the respective .csv files.\n",
    "    path = 'MNIST'\n",
    "    df_X_train = pd.read_csv(f'{path}/X_train.csv', header=None)\n",
    "    df_y_train = pd.read_csv(f'{path}/y_train.csv', header=None)\n",
    "    df_X_test = pd.read_csv(f'{path}/X_test.csv', header=None)\n",
    "    df_y_test = pd.read_csv(f'{path}/y_test.csv', header=None)\n",
    "\n",
    "    # save in numpy arrays\n",
    "    X_train_raw = df_X_train.to_numpy()\n",
    "    y_train_raw = df_y_train.to_numpy()\n",
    "    X_test_raw = df_X_test.to_numpy()\n",
    "    y_test_raw = df_y_test.to_numpy()\n",
    "\n",
    "    # get training set size\n",
    "    n_train = X_train_raw.shape[0]\n",
    "    n_test = X_test_raw.shape[0]\n",
    "\n",
    "    # normalize all features to [0,1]\n",
    "    X_all = MyUtils.normalize_0_1(np.concatenate((X_train_raw, X_test_raw), axis=0))\n",
    "    X_train = X_all[:n_train]\n",
    "    X_test = X_all[n_train:]\n",
    "\n",
    "    # convert each label into a 0-1 vector\n",
    "    y_train = np.zeros((n_train, k))\n",
    "    y_test = np.zeros((n_test, k))\n",
    "    for i in range(n_train):\n",
    "        y_train[i,int(y_train_raw[i])] = 1.0\n",
    "    for i in range(n_test):\n",
    "        y_test[i,int(y_test_raw[i])] = 1.0\n",
    "\n",
    "    #Insure that the data correctly loaded in.\n",
    "    assert X_train.shape == (60000, 784), \"Incorrect input, expected (60000, 784), found \" + X_train.shape\n",
    "    assert y_train.shape == (60000, 10), \"Incorrect input, expected (60000, 10), found \" + y_train.shape\n",
    "    assert X_test.shape  == (10000, 784), \"Incorrect input, expected (10000, 784), found \" + X_test.shape\n",
    "    assert y_test.shape  == (10000, 10), \"Incorrect input, expected (10000, 10), found \" + y_test.shape\n",
    "\n",
    "    return (X_train,y_train,X_test,y_test)\n",
    "\n",
    "def load_weights(file=\"../seeded_weights.npz\"):\n",
    "    container = np.load(file)\n",
    "    weight_list = [container[key] for key in container]\n",
    "    return weight_list\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c497c409",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25176/2087950633.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mX_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnuts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Forward_Feed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_forward_feed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25176/2087950633.py\u001b[0m in \u001b[0;36mtest_forward_feed\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mseed_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mX_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnuts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Forward_Feed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_forward_feed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25176/2366734114.py\u001b[0m in \u001b[0;36m_Forward_Feed\u001b[1;34m(self, layer_id)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[1;31m# the previous layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m         \u001b[0mp_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer_id\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[1;31m# the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77474a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######### INITIAL RESULTS #########\n",
      "passed_math_util: True, passed_add_layer: True, passed_init_weights: True\n",
      "######### TRAINING RESULTS - Model error #########\n",
      "Train: [0.0281], Test: [0.0324]\n",
      "test_saved_weights: SUCCESS!!!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "verbose = True\n",
    "\n",
    "def main():\n",
    "    passed_math_util = passed_add_layer = passed_init_weights = passed_seeded_weights = passed_test_fit = passed_test_weights = False\n",
    "\n",
    "    passed_math_util = test_math_util()\n",
    "    passed_add_layer = test_add_layer()\n",
    "    passed_init_weights = test_init_weights()\n",
    "\n",
    "    print(f\"\\n######### INITIAL RESULTS #########\\npassed_math_util: {passed_math_util}, passed_add_layer: {passed_add_layer}, passed_init_weights: {passed_init_weights}\")\n",
    "\n",
    "    if not (passed_math_util and passed_add_layer and passed_init_weights):\n",
    "        print(\"Stopping tester due to failed test of a fundamental functionality.\\nPlease review the tester that failed.\")\n",
    "        return\n",
    "\n",
    "    test_fit() #Checks to see if the fit method crashes from running a handful of iterations.\n",
    "\n",
    "    train_and_save_weights() #Trains the weights and saves them to \"weights.npz\". Expected validation error is less than 0.04 or 4%.\n",
    "    \n",
    "    test_saved_weights() #Verifies the accuracy of the model's resulting weights\n",
    "    \n",
    "def test_fit():\n",
    "    (X_train,y_train,X_test,y_test) = loadData()\n",
    "    nuts = _createNN()\n",
    "    nuts.fit(X_train, y_train, eta = 0.1, iterations = 5, SGD = True, mini_batch_size = 20)\n",
    "\n",
    "def test_saved_weights(file = \"weights.npz\"):\n",
    "    (X_train,y_train,X_test,y_test) = loadData()\n",
    "\n",
    "    #loads weights into nuts\n",
    "    npz_weights = load_weights(file=file)\n",
    "    nuts = _createNN()\n",
    "    _import_weights(npz_weights, nuts)\n",
    "\n",
    "    train_error = nuts.error(X_train, y_train)\n",
    "    test_error  = nuts.error(X_test,  y_test)\n",
    "\n",
    "    print(f\"######### TRAINING RESULTS - Model error #########\\nTrain: {np.round(train_error, 4)}, Test: {np.round(test_error, 4)}\")\n",
    "    if test_error > 0.00001 and test_error < 0.05:\n",
    "        print(\"test_saved_weights: SUCCESS!!!\")\n",
    "    else:\n",
    "        print(f\"test_saved_weights: Insufficient model accuracy. Expcected error less than 0.05 or 5%\\nActual test error: {np.round(test_error, 4)}\")\n",
    "    if test_error <= 0.00001:\n",
    "        print(\"test_saved_weights: Test error is suspiciously low. Please reevaluate your error method.\")\n",
    "    return False\n",
    "\n",
    "def _import_weights(npz_weights, nuts):\n",
    "    for ell in range(1, nuts.L+1):\n",
    "        nuts.layers[ell].W = np.array(npz_weights[ell - 1])\n",
    "\n",
    "def train_and_save_weights():\n",
    "    (X_train,y_train,X_test,y_test) = loadData()\n",
    "    nuts = _createNN()\n",
    "    nuts.fit(X_train, y_train, eta = 0.1, iterations = 10000, SGD = True, mini_batch_size = 20)\n",
    "    _save_weights(nuts)\n",
    "\n",
    "def _save_weights(nuts):\n",
    "    file = \"weights.npz\"\n",
    "    clearZ(file=file)\n",
    "    weights_list = []\n",
    "\n",
    "    for ell in range(1, nuts.L+1):\n",
    "        cur_layer = nuts.layers[ell]\n",
    "        weights_list.append(cur_layer.W)\n",
    "\n",
    "    saveAllZ(weights_list, file=file)\n",
    "\n",
    "def _createNN(k = 10, d = 784):\n",
    "    nuts = NeuralNetwork()\n",
    "    nuts.add_layer(d = d)  # input layer - 0\n",
    "    nuts.add_layer(d = 100, act = 'relu')  # hidden layer - 1\n",
    "    nuts.add_layer(d = 30, act = 'relu')  # hiddent layer - 2\n",
    "    nuts.add_layer(d = k, act = 'logis')  # output layer,    multi-class classification, #classes = k\n",
    "    return nuts\n",
    "\n",
    "def test_seeded_weights():\n",
    "    d = 2\n",
    "    k = 2\n",
    "    passed = True\n",
    "\n",
    "    # build the network\n",
    "    nuts = NeuralNetwork()\n",
    "\n",
    "    nuts.add_layer(d = d)  # input layer - 0\n",
    "    nuts.add_layer(d = 5, act = 'relu')  # hidden layer - 1\n",
    "    nuts.add_layer(d = k, act = 'logis')  # output layer\n",
    "\n",
    "    nuts._init_weights()\n",
    "\n",
    "    seed_weights = load_weights()\n",
    "    for layer, seed_weight in zip(nuts.layers[1:],seed_weights):\n",
    "        seed_weight = np.array(seed_weight)\n",
    "        weight = np.array(layer.W)\n",
    "\n",
    "        if (weight != seed_weight).any():\n",
    "            if verbose:\n",
    "                print(f\"check_seeded_weights:\\nExpected:\\n{seed_weight}\\nFound:\\n{weight}\")\n",
    "            passed = False\n",
    "\n",
    "    return passed\n",
    "\n",
    "def test_init_weights():\n",
    "    d = 10\n",
    "    k = 8\n",
    "    passed = True\n",
    "\n",
    "    # build the network\n",
    "    nuts = NeuralNetwork()\n",
    "\n",
    "    nuts.add_layer(d = d)  # input layer - 0\n",
    "    nuts.add_layer(d = 5, act = 'relu')  # hidden layer - 1\n",
    "    nuts.add_layer(d = k, act = 'logis')  # output layer\n",
    "\n",
    "    nuts._init_weights()\n",
    "\n",
    "    #Check dimensionality of weights\n",
    "    if nuts.layers[0].W != None:\n",
    "        print(\"\")\n",
    "\n",
    "    shapes = [(11,5),(6,8)]\n",
    "    for layer, dim in zip(nuts.layers[1:], shapes):\n",
    "        if layer.W.shape != dim:\n",
    "            if verbose:\n",
    "                print(f\"test_init_weights: Invalid dimensions of the instantiated weights. Expected {dim}, found {layer.W.shape}\")\n",
    "            passed = False\n",
    "\n",
    "    return passed\n",
    "\n",
    "def test_add_layer():\n",
    "    # build the network\n",
    "    nuts = NeuralNetwork()\n",
    "\n",
    "    assert nuts.L == -1, f\"After initialization, L = -1. Found L = {nuts.L}\"\n",
    "\n",
    "    passed = True\n",
    "\n",
    "    nuts.add_layer(d = 5, act = 'logis')\n",
    "    if nuts.L != 0:\n",
    "        if verbose:\n",
    "            print(f\"test_add_layer: After adding a layer, L = 0. Found L = {nuts.L}\")\n",
    "        passed = False\n",
    "    if len(nuts.layers) != 1:\n",
    "        if verbose:\n",
    "            print(f\"test_add_layer: Failed to add layer to layers.\")\n",
    "        passed = False\n",
    "\n",
    "    return passed\n",
    "\n",
    "def test_math_util():\n",
    "    passed = [test_tanh(), test_tanh_de(), test_logis(), test_logis_de(), test_iden(), test_iden_de(), test_relu(), test_relu_de()]\n",
    "    if all(passed):\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"test_math_util passed methods:\\n\\\n",
    "tanh: {passed[0]}, tanh_de: {passed[1]}\\n\\\n",
    "logis: {passed[2]}, logis_de: {passed[3]}\\n\\\n",
    "iden: {passed[4]}, iden_de: {passed[5]}\\n\\\n",
    "relu: {passed[6]}, relu_de: {passed[7]}\")\n",
    "    \n",
    "        return False\n",
    "\n",
    "def test_tanh():\n",
    "    x = [0.0,1.0,2.0,-1.0]\n",
    "    y = [0.0,0.761594156,0.9640275801,-0.761594156]\n",
    "    y_hat = MyMath.tanh(x)\n",
    "    return _test_math_util(x,y,y_hat,\"tanh\")\n",
    "\n",
    "def test_tanh_de():\n",
    "    x = [0.0,1.0,2.0,-1.0]\n",
    "    y = [1,0.419974,0.0706508,0.419974]\n",
    "    y_hat = MyMath.tanh_de(x)\n",
    "    return _test_math_util(x,y,y_hat,\"tanh_de\")\n",
    "\n",
    "def test_logis():\n",
    "    x = [0.0,1.0,2.0,-1.0]\n",
    "    y = [0.5,0.7310586,0.8807971,0.2689414]\n",
    "    y_hat = MyMath.logis(x)\n",
    "    return _test_math_util(x,y,y_hat,\"logis\")\n",
    "\n",
    "def test_logis_de():\n",
    "    x = [0.0,1.0,2.0,-1.0]\n",
    "    y = [0.25,0.196612,0.104994,0.196612]\n",
    "    y_hat = MyMath.logis_de(x)\n",
    "    return _test_math_util(x,y,y_hat,\"logis_de\")\n",
    "\n",
    "def test_iden():\n",
    "    x = [0.0,1.0,2.0,-1.0]\n",
    "    y = [0.0,1.0,2.0,-1.0]\n",
    "    y_hat = MyMath.iden(x)\n",
    "    return _test_math_util(x,y,y_hat,\"iden\")\n",
    "\n",
    "def test_iden_de():\n",
    "    x = [0.0,1.0,2.0,-1.0]\n",
    "    y = [1,1,1,1]\n",
    "    y_hat = MyMath.iden_de(x)\n",
    "    return _test_math_util(x,y,y_hat,\"iden_de\")\n",
    "\n",
    "def test_relu():\n",
    "    x = [0.0,1.0,2.0,-1.0]\n",
    "    y = [0.0,1.0,2.0,0.0]\n",
    "    y_hat = MyMath.relu(x)\n",
    "    return _test_math_util(x,y,y_hat,\"relu\")\n",
    "\n",
    "def test_relu_de():\n",
    "    x = [0.0,1.0,2.0,-1.0]\n",
    "    y = [0,1,1,0]\n",
    "    y_hat = MyMath.relu_de(x)\n",
    "    return _test_math_util(x,y,y_hat,\"relu_de\")\n",
    "\n",
    "def _test_math_util(x,y,y_hat,name):\n",
    "\n",
    "    passed = True\n",
    "\n",
    "    if not _is_numpy_array(y_hat):\n",
    "        if verbose:\n",
    "            print(f\"Incorrect return type. Expected type {np.ndarray}, but found {type(y_hat)}\")\n",
    "        passed = False\n",
    "\n",
    "    for y, y_hat, x in zip(y, y_hat, x):\n",
    "        if not inThreshold(y,y_hat,0.00001):\n",
    "            if verbose:\n",
    "                print(f\"Incorrect {name} value, expected {y}, but found {y_hat} for x = {x}\")\n",
    "            passed = False\n",
    "    return passed\n",
    "\n",
    "def _is_numpy_array(x):\n",
    "    return isinstance(x,np.ndarray)\n",
    "\n",
    "def inThreshold(x1, x2, threshold=1.1):\n",
    "    return abs(x1 - x2) < threshold\n",
    "\n",
    "def saveAllZ(Z_list,file=\"output.npz\"):\n",
    "    np.savez(file,*Z_list)\n",
    "\n",
    "def clearZ(file=\"output.npz\"):\n",
    "    f = open(file, \"w\")\n",
    "    f.close()\n",
    "\n",
    "def loadData():\n",
    "    k = 10\n",
    "    d = 784\n",
    "\n",
    "    #Reads the files into pandas dataframes from the respective .csv files.\n",
    "    path = 'MNIST'\n",
    "    df_X_train = pd.read_csv(f'{path}/X_train.csv', header=None)\n",
    "    df_y_train = pd.read_csv(f'{path}/y_train.csv', header=None)\n",
    "    df_X_test = pd.read_csv(f'{path}/X_test.csv', header=None)\n",
    "    df_y_test = pd.read_csv(f'{path}/y_test.csv', header=None)\n",
    "\n",
    "    # save in numpy arrays\n",
    "    X_train_raw = df_X_train.to_numpy()\n",
    "    y_train_raw = df_y_train.to_numpy()\n",
    "    X_test_raw = df_X_test.to_numpy()\n",
    "    y_test_raw = df_y_test.to_numpy()\n",
    "\n",
    "    # get training set size\n",
    "    n_train = X_train_raw.shape[0]\n",
    "    n_test = X_test_raw.shape[0]\n",
    "\n",
    "    # normalize all features to [0,1]\n",
    "    X_all = MyUtils.normalize_0_1(np.concatenate((X_train_raw, X_test_raw), axis=0))\n",
    "    X_train = X_all[:n_train]\n",
    "    X_test = X_all[n_train:]\n",
    "\n",
    "    # convert each label into a 0-1 vector\n",
    "    y_train = np.zeros((n_train, k))\n",
    "    y_test = np.zeros((n_test, k))\n",
    "    for i in range(n_train):\n",
    "        y_train[i,int(y_train_raw[i])] = 1.0\n",
    "    for i in range(n_test):\n",
    "        y_test[i,int(y_test_raw[i])] = 1.0\n",
    "\n",
    "    #Insure that the data correctly loaded in.\n",
    "    assert X_train.shape == (60000, 784), \"Incorrect input, expected (60000, 784), found \" + X_train.shape\n",
    "    assert y_train.shape == (60000, 10), \"Incorrect input, expected (60000, 10), found \" + y_train.shape\n",
    "    assert X_test.shape  == (10000, 784), \"Incorrect input, expected (10000, 784), found \" + X_test.shape\n",
    "    assert y_test.shape  == (10000, 10), \"Incorrect input, expected (10000, 10), found \" + y_test.shape\n",
    "\n",
    "    return (X_train,y_train,X_test,y_test)\n",
    "\n",
    "def load_weights(file=\"seeded_weights.npz\"):\n",
    "    container = np.load(file)\n",
    "    weight_list = [container[key] for key in container]\n",
    "    return weight_list\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4650890c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
